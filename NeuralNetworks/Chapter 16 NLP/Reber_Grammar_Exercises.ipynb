{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f310209e",
   "metadata": {},
   "source": [
    "Embedded Reber grammars were used by Hochreiter and Schmidhuber in their paper about LSTMs. They are artificial grammars that produce strings such as “BPBTSXXVPSEPE”. Check out Jenny Orr’s nice introduction to this topic, then choose a particular embedded Reber grammar (such as the one represented on Orr’s page), then train an RNN to identify whether a string respects that grammar or not. You will first need to write a function capable of generating a training batch containing about 50% strings that respect the grammar, and 50% that don’t."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f146889",
   "metadata": {},
   "source": [
    "First i wrote the algorithm using three classes to represent the graph and the random next step choice. Without the embedded option, just simple phrases. And the method to validate algorithmically a sequence. Now i have the capability of generating valid sentences. Lets try to generate wrong ones randomly, but in order to the model to learn all the rules i must include the right mistakes in the sentences such as:\n",
    "\n",
    "- Wrong char: Switch a random wrong char in a random position\n",
    "- Invalid Transition: In order to the model to understand the state a wrong transition can be generated \n",
    "- Incomplete sequences: The model must acknoledge the correct ending of the sequence\n",
    "- Extra chars: The model must recognize a wrong end of sequence\n",
    "- Permutation: Change the order of correct sub-sequences making the model understand the order logic\n",
    "\n",
    "I have a background in object oriented programming so i made it in classes. Now that i can create wrong and right sentences. I must prepare a dataset to train my model.\n",
    "\n",
    "I was lazy so i asked the ia if it could generate me a class to generate a dataset leveraging TensorFlow Dataset API, it made a quite good job, maybe too much for this experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27078a6",
   "metadata": {},
   "source": [
    "I was doubing about the use of prefetch, since im not loading anything from files, the generation of sequences is fast, done in memory, the sequences are short, there are no calls to the I/O from disc o heavy processing. I found out that pre-fetch can still be useful to pad and batch the sentences, filling variable length sentences can take some time, and if useing pre-fetch can leverage a GPU if used, while the model trains a batch, prefetch loads the next batched/padded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6014c9",
   "metadata": {},
   "source": [
    "While programming i forgot something very important: To set the tensor flow seed. This is important because it allows the developer to really detect valuable changes in the model, since working with always the same randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e8bd09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataset classes\n",
    "\n",
    "from random import randint, choice\n",
    "\n",
    "\n",
    "class ReberGraph:\n",
    "    def __init__(self, embedded=False):\n",
    "        self.embedded = embedded\n",
    "        self.result_string = ''\n",
    "        self.initial_node = ReberNode()\n",
    "        second_node = ReberNode()\n",
    "        third_node = ReberNode()\n",
    "        fourth_node = ReberNode()\n",
    "        fifth_node = ReberNode()\n",
    "        sixth_node = ReberNode()\n",
    "        seventh_node = ReberNode()\n",
    "        final_node = ReberNode(is_terminal=True)\n",
    "\n",
    "        self.initial_node.add_connection(ReberConnection(self.initial_node, second_node, 'B'))\n",
    "        \n",
    "        second_node.add_connection(ReberConnection(second_node, third_node, 'T'))        \n",
    "        second_node.add_connection(ReberConnection(second_node, fourth_node, 'P'))\n",
    "        \n",
    "        third_node.add_connection(ReberConnection(third_node, third_node, 'S'))\n",
    "        third_node.add_connection(ReberConnection(third_node, fifth_node, 'X'))\n",
    "\n",
    "        fourth_node.add_connection(ReberConnection(fourth_node, sixth_node, 'V'))\n",
    "        fourth_node.add_connection(ReberConnection(fourth_node, fourth_node, 'T'))\n",
    "\n",
    "        fifth_node.add_connection(ReberConnection(fifth_node, fourth_node, 'X'))\n",
    "        fifth_node.add_connection(ReberConnection(fifth_node, seventh_node, 'S'))\n",
    "\n",
    "        sixth_node.add_connection(ReberConnection(sixth_node, fifth_node, 'V'))\n",
    "        sixth_node.add_connection(ReberConnection(sixth_node, seventh_node, 'P'))\n",
    "\n",
    "        seventh_node.add_connection(ReberConnection(seventh_node, final_node, 'E'))\n",
    "        \n",
    "        self.error_strategies = []\n",
    "\n",
    "    def set_error_strategies(self, strategies):\n",
    "        self.error_strategies = strategies\n",
    "\n",
    "    def generate_sequence(self):\n",
    "        sequence = ''\n",
    "        current_node = self.initial_node\n",
    "        while not current_node.is_terminal:\n",
    "            selected_connection_index = randint(0, len(current_node.connections) - 1)\n",
    "            sequence += current_node.connections[selected_connection_index].label\n",
    "            current_node = current_node.connections[selected_connection_index].node_to\n",
    "        \n",
    "        return sequence   \n",
    "\n",
    "    def validate_sequence(self, sequence):\n",
    "        current_node = self.initial_node\n",
    "        for char in sequence:\n",
    "            found = False\n",
    "            for connection in current_node.connections:\n",
    "                if connection.label == char:\n",
    "                    current_node = connection.node_to\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                return False\n",
    "        return current_node.is_terminal\n",
    "\n",
    "    def generate_wrong_sequence(self):\n",
    "        selected_strategy = choice(self.error_strategies)\n",
    "        return selected_strategy.generate_error(self)\n",
    "\n",
    "        \n",
    "class ReberConnection:\n",
    "    def __init__(self, node_from, node_to, label):\n",
    "        self.node_from = node_from\n",
    "        self.node_to = node_to\n",
    "        self.label = label\n",
    "\n",
    "class ReberNode:\n",
    "    def __init__(self, is_terminal=False):\n",
    "        self.is_terminal = is_terminal\n",
    "        self.connections = []\n",
    "\n",
    "    def add_connection(self, connection):\n",
    "        self.connections.append(connection)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28a7ef23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTXSE\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "reber_graph = ReberGraph()\n",
    "sequence = reber_graph.generate_sequence()\n",
    "print(sequence)\n",
    "print(reber_graph.validate_sequence(sequence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dcf2766",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from random import randint, choice\n",
    "\n",
    "\n",
    "class SequenceErrorStrategy(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def generate_error(self, graph):\n",
    "        pass\n",
    "\n",
    "\n",
    "class WrongCharError(SequenceErrorStrategy):    \n",
    "    \n",
    "    def generate_error(self, graph):\n",
    "        sequence = graph.generate_sequence()\n",
    "        if len(sequence) <= 1:\n",
    "            return sequence\n",
    "        \n",
    "        invalid_chars = ['Z', 'Q', 'W', 'R', 'Y', 'U', 'I', 'O', 'A', 'C', 'D', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N']\n",
    "        random_pos = randint(1, len(sequence) - 2)\n",
    "        wrong_char = choice(invalid_chars)\n",
    "        \n",
    "        return sequence[:random_pos] + wrong_char + sequence[random_pos + 1:]\n",
    "\n",
    "\n",
    "class InvalidTransitionError(SequenceErrorStrategy):\n",
    "    \n",
    "    def generate_error(self, graph):\n",
    "        sequence = ''\n",
    "        current_node = graph.initial_node\n",
    "        valid_labels = {'B', 'T', 'P', 'S', 'X', 'V', 'E'}\n",
    "        \n",
    "        steps_before_error = randint(1, 4)\n",
    "        step_count = 0\n",
    "        \n",
    "        while not current_node.is_terminal and len(sequence) < 15:\n",
    "            step_count += 1\n",
    "            \n",
    "            if step_count == steps_before_error and len(current_node.connections) > 0:\n",
    "                available_labels = {c.label for c in current_node.connections}\n",
    "                invalid_labels = list(valid_labels - available_labels)\n",
    "                \n",
    "                if invalid_labels:\n",
    "                    wrong_label = choice(invalid_labels)\n",
    "                    sequence += wrong_label\n",
    "                    return sequence\n",
    "            \n",
    "            if len(current_node.connections) == 0:\n",
    "                break\n",
    "            selected_connection_index = randint(0, len(current_node.connections) - 1)\n",
    "            sequence += current_node.connections[selected_connection_index].label\n",
    "            current_node = current_node.connections[selected_connection_index].node_to\n",
    "        \n",
    "        return sequence if not current_node.is_terminal else sequence[:-1]\n",
    "\n",
    "\n",
    "class IncompleteSequenceError(SequenceErrorStrategy):\n",
    "    \n",
    "    def generate_error(self, graph):\n",
    "        sequence = ''\n",
    "        current_node = graph.initial_node\n",
    "        max_steps = randint(2, 5)\n",
    "        step_count = 0\n",
    "        \n",
    "        while not current_node.is_terminal and step_count < max_steps:\n",
    "            if len(current_node.connections) == 0:\n",
    "                break\n",
    "            selected_connection_index = randint(0, len(current_node.connections) - 1)\n",
    "            sequence += current_node.connections[selected_connection_index].label\n",
    "            current_node = current_node.connections[selected_connection_index].node_to\n",
    "            step_count += 1\n",
    "        \n",
    "        return sequence if len(sequence) > 0 else 'BP'\n",
    "\n",
    "\n",
    "class ExtraCharsError(SequenceErrorStrategy):\n",
    "    \n",
    "    def generate_error(self, graph):\n",
    "        sequence = graph.generate_sequence()\n",
    "        extra_chars = ['B', 'T', 'P', 'S', 'X', 'V']\n",
    "        num_extra = randint(1, 3)\n",
    "        \n",
    "        for _ in range(num_extra):\n",
    "            sequence += choice(extra_chars)\n",
    "        \n",
    "        return sequence\n",
    "\n",
    "\n",
    "class WrongStartError(SequenceErrorStrategy):    \n",
    "    \n",
    "    def generate_error(self, graph):\n",
    "        invalid_starts = ['T', 'P', 'S', 'X', 'V', 'E']\n",
    "        wrong_start = choice(invalid_starts)\n",
    "        \n",
    "        sequence = ''\n",
    "        current_node = graph.initial_node\n",
    "        \n",
    "        if len(current_node.connections) > 0:\n",
    "            first_connection = current_node.connections[0]\n",
    "            current_node = first_connection.node_to\n",
    "        \n",
    "        while not current_node.is_terminal and len(sequence) < 10:\n",
    "            if len(current_node.connections) == 0:\n",
    "                break\n",
    "            selected_connection_index = randint(0, len(current_node.connections) - 1)\n",
    "            sequence += current_node.connections[selected_connection_index].label\n",
    "            current_node = current_node.connections[selected_connection_index].node_to\n",
    "        \n",
    "        return wrong_start + sequence\n",
    "\n",
    "\n",
    "class MissingMiddleError(SequenceErrorStrategy):\n",
    "    \n",
    "    def generate_error(self, graph):\n",
    "        sequence = graph.generate_sequence()\n",
    "        if len(sequence) <= 3:\n",
    "            return sequence[:-1]\n",
    "        \n",
    "        pos_to_remove = randint(1, len(sequence) - 2)\n",
    "        return sequence[:pos_to_remove] + sequence[pos_to_remove + 1:]\n",
    "\n",
    "\n",
    "class SwapNonAdjacentError(SequenceErrorStrategy):\n",
    "    \n",
    "    def generate_error(self, graph):\n",
    "        sequence = list(graph.generate_sequence())\n",
    "        if len(sequence) < 4:\n",
    "            return ''.join(sequence[::-1])\n",
    "        \n",
    "        pos1 = randint(1, len(sequence) - 3)\n",
    "        pos2 = randint(pos1 + 2, len(sequence) - 1)\n",
    "        sequence[pos1], sequence[pos2] = sequence[pos2], sequence[pos1]\n",
    "        \n",
    "        return ''.join(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18d160b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Valid sequences ===\n",
      "Valid 1: BPVVXVPE (validation: True)\n",
      "Valid 2: BPTTTVPE (validation: True)\n",
      "Valid 3: BTSSXSE (validation: True)\n",
      "\n",
      "=== Invalid sequences ===\n",
      "Invalid 1: PPVPE (validation: False)\n",
      "Invalid 2: BTXXTTVPE (validation: True)\n",
      "Invalid 3: BTSXSE (validation: True)\n",
      "Invalid 4: BTXSEVS (validation: False)\n",
      "Invalid 5: BSXTE (validation: False)\n"
     ]
    }
   ],
   "source": [
    "# Usage example: Configure and test error strategies\n",
    "\n",
    "error_strategies = [\n",
    "    WrongCharError(),\n",
    "    InvalidTransitionError(),\n",
    "    IncompleteSequenceError(),\n",
    "    ExtraCharsError(),\n",
    "    WrongStartError(),\n",
    "    MissingMiddleError(),\n",
    "    SwapNonAdjacentError()\n",
    "]\n",
    "\n",
    "reber_graph.set_error_strategies(error_strategies)\n",
    "\n",
    "print(\"=== Valid sequences ===\")\n",
    "for i in range(3):\n",
    "    valid_seq = reber_graph.generate_sequence()\n",
    "    print(f\"Valid {i+1}: {valid_seq} (validation: {reber_graph.validate_sequence(valid_seq)})\")\n",
    "\n",
    "print(\"\\n=== Invalid sequences ===\")\n",
    "for i in range(5):\n",
    "    wrong_seq = reber_graph.generate_wrong_sequence()\n",
    "    print(f\"Invalid {i+1}: {wrong_seq} (validation: {reber_graph.validate_sequence(wrong_seq)})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37919dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ReberDatasetFixed definido correctamente\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from typing import Tuple, Optional\n",
    "import random\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "class ReberDataset:\n",
    "    def __init__(self, graph: ReberGraph, vocab: Optional[dict] = None):\n",
    "        self.graph = graph\n",
    "        self.reber_alphabet = ['B', 'T', 'P', 'S', 'X', 'V', 'E']\n",
    "        self.invalid_chars = ['Z', 'Q', 'W', 'R', 'Y', 'U', 'I', 'O', 'A', 'C', 'D', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N']\n",
    "        \n",
    "        if vocab is None:\n",
    "            self.vocab = {'<PAD>': 0}\n",
    "            for idx, char in enumerate(self.reber_alphabet, start=1):\n",
    "                self.vocab[char] = idx\n",
    "            for idx, char in enumerate(self.invalid_chars, start=len(self.reber_alphabet) + 1):\n",
    "                self.vocab[char] = idx\n",
    "        else:\n",
    "            self.vocab = vocab\n",
    "        \n",
    "        self.idx_to_char = {idx: char for char, idx in self.vocab.items()}\n",
    "        self.vocab_size = len(self.vocab)\n",
    "    \n",
    "    def _sequence_to_indices(self, sequence: str) -> list:\n",
    "        return [self.vocab.get(char, 0) for char in sequence]\n",
    "    \n",
    "    def _generate_guaranteed_invalid(self) -> str:\n",
    "        \"\"\"Generates a guaranteed invalid sequence\"\"\"\n",
    "        max_attempts = 50\n",
    "        \n",
    "        for _ in range(max_attempts):\n",
    "            sequence = self.graph.generate_wrong_sequence()\n",
    "            if not self.graph.validate_sequence(sequence):\n",
    "                return sequence\n",
    "        \n",
    "        valid_seq = self.graph.generate_sequence()\n",
    "        if len(valid_seq) > 2:\n",
    "            pos = len(valid_seq) // 2\n",
    "            return valid_seq[:pos] + 'Z' + valid_seq[pos+1:]\n",
    "        return \"BZE\"\n",
    "    \n",
    "    def _generate_sample(self, is_valid: bool) -> Tuple[list, int]:\n",
    "        if is_valid:\n",
    "            sequence = self.graph.generate_sequence()\n",
    "            label = 1\n",
    "        else:\n",
    "            sequence = self._generate_guaranteed_invalid()\n",
    "            label = 0\n",
    "        \n",
    "        tokenized = self._sequence_to_indices(sequence)\n",
    "        return tokenized, label\n",
    "    \n",
    "    def _generator(self, num_samples: int, valid_ratio: float = 0.5):\n",
    "        num_valid = int(num_samples * valid_ratio)\n",
    "        num_invalid = num_samples - num_valid\n",
    "        \n",
    "        samples_order = [True] * num_valid + [False] * num_invalid\n",
    "        random.shuffle(samples_order)\n",
    "        \n",
    "        for is_valid in samples_order:\n",
    "            tokenized, label = self._generate_sample(is_valid=is_valid)\n",
    "            yield (np.array(tokenized, dtype=np.int32), np.array(label, dtype=np.int32))\n",
    "    \n",
    "    def generate_dataset(\n",
    "        self,\n",
    "        num_samples: int = 10000,\n",
    "        batch_size: int = 32,\n",
    "        valid_ratio: float = 0.5,\n",
    "        max_length: Optional[int] = None,\n",
    "        shuffle: bool = True,\n",
    "        shuffle_buffer_size: int = 10000,\n",
    "        prefetch: bool = True,\n",
    "        prefetch_size: int = tf.data.AUTOTUNE,\n",
    "        cache: bool = False,\n",
    "        repeat: bool = False,\n",
    "    ) -> tf.data.Dataset:\n",
    "        dataset = tf.data.Dataset.from_generator(\n",
    "            lambda: self._generator(num_samples, valid_ratio),\n",
    "            output_signature=(\n",
    "                tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
    "                tf.TensorSpec(shape=(), dtype=tf.int32)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(buffer_size=shuffle_buffer_size, reshuffle_each_iteration=True)\n",
    "        \n",
    "        dataset = dataset.padded_batch(\n",
    "            batch_size=batch_size,\n",
    "            padded_shapes=([max_length] if max_length else [None], []),\n",
    "            padding_values=(self.vocab['<PAD>'], 0),\n",
    "            drop_remainder=False\n",
    "        )\n",
    "        \n",
    "        if cache:\n",
    "            dataset = dataset.cache()\n",
    "        if prefetch:\n",
    "            dataset = dataset.prefetch(prefetch_size)\n",
    "        if repeat:\n",
    "            dataset = dataset.repeat()\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    def generate_train_val_split(\n",
    "        self,\n",
    "        num_samples: int = 10000,\n",
    "        val_ratio: float = 0.2,\n",
    "        batch_size: int = 32,\n",
    "        **kwargs\n",
    "    ) -> Tuple[tf.data.Dataset, tf.data.Dataset]:\n",
    "        num_train = int(num_samples * (1 - val_ratio))\n",
    "        num_val = num_samples - num_train\n",
    "        \n",
    "        train_dataset = self.generate_dataset(num_samples=num_train, batch_size=batch_size, **kwargs)\n",
    "        val_dataset = self.generate_dataset(\n",
    "            num_samples=num_val, batch_size=batch_size, shuffle=False, cache=False,\n",
    "            **{k: v for k, v in kwargs.items() if k not in ['shuffle', 'cache']}\n",
    "        )\n",
    "        \n",
    "        return train_dataset, val_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66c92f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VERIFICACIÓN DEL DATASET CORREGIDO ===\n",
      "\n",
      "Total muestras verificadas: 1600\n",
      "Label 0 (inválidas): 760 (47.5%)\n",
      "Label 1 (válidas): 840 (52.5%)\n",
      "\n",
      "Consistencia label-validación: 1600/1600 (100.0%)\n"
     ]
    }
   ],
   "source": [
    "# Verify that the dataset is balanced\n",
    "\n",
    "dataset_generator = ReberDataset(reber_graph)\n",
    "\n",
    "train_ds, val_ds = dataset_generator.generate_train_val_split(\n",
    "    num_samples=10000,\n",
    "    val_ratio=0.2,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    prefetch=True\n",
    ")\n",
    "\n",
    "\n",
    "label_counts = {0: 0, 1: 0}\n",
    "correct_labels = 0\n",
    "total = 0\n",
    "\n",
    "for sequences, labels in train_ds.take(50):\n",
    "    for i in range(len(sequences)):\n",
    "        seq_str = ''.join([dataset_generator.idx_to_char.get(int(idx), '') \n",
    "                          for idx in sequences[i].numpy() if idx != 0])\n",
    "        label = int(labels[i].numpy())\n",
    "        is_valid = reber_graph.validate_sequence(seq_str)\n",
    "        \n",
    "        label_counts[label] += 1\n",
    "        total += 1\n",
    "        \n",
    "        # Verify consistency: label must match actual validation\n",
    "        expected_valid = (label == 1)\n",
    "        if is_valid == expected_valid:\n",
    "            correct_labels += 1\n",
    "\n",
    "print(f\"Total verified samples: {total}\")\n",
    "print(f\"Label 0 (invalid): {label_counts[0]} ({100*label_counts[0]/total:.1f}%)\")\n",
    "print(f\"Label 1 (valid): {label_counts[1]} ({100*label_counts[1]/total:.1f}%)\")\n",
    "print(f\"\\nLabel-validation consistency: {correct_labels}/{total} ({100*correct_labels/total:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e668616",
   "metadata": {},
   "source": [
    "Now lets train a model and thee how it goes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4018487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard logs will be saved to: logs/fit/20251201-205332\n",
      "\n",
      "To start TensorBoard, run in terminal:\n",
      "tensorboard --logdir=logs/fit\n",
      "\n",
      "Or use the magic command:\n",
      "%tensorboard --logdir logs/fit\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import os\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "log_dir = os.path.join(\"logs\", \"fit\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "    histogram_freq=1,  # Log weight histograms every epoch\n",
    "    write_graph=True,  # Visualize the computation graph\n",
    "    write_images=True,  # Write model weights as images\n",
    "    update_freq='epoch',  # Log metrics every epoch\n",
    "    profile_batch=0,  # Disable profiling (set to batch number to enable)\n",
    "    embeddings_freq=0,  # Frequency to visualize embeddings (0 = disabled)\n",
    ")\n",
    "\n",
    "print(f\"TensorBoard logs will be saved to: {log_dir}\")\n",
    "print(\"\\nTo start TensorBoard, run in terminal:\")\n",
    "print(f\"tensorboard --logdir={os.path.dirname(log_dir)}\")\n",
    "print(\"\\nOr use the magic command:\")\n",
    "print(f\"%tensorboard --logdir {os.path.dirname(log_dir)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf80ac78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "250/250 [==============================] - 5s 11ms/step - loss: 0.4780 - accuracy: 0.7732 - val_loss: 0.3084 - val_accuracy: 0.9020\n",
      "Epoch 2/10\n",
      "250/250 [==============================] - 3s 8ms/step - loss: 0.2025 - accuracy: 0.9367 - val_loss: 0.1824 - val_accuracy: 0.9390\n",
      "Epoch 3/10\n",
      "250/250 [==============================] - 3s 9ms/step - loss: 0.1671 - accuracy: 0.9492 - val_loss: 0.1557 - val_accuracy: 0.9575\n",
      "Epoch 4/10\n",
      "250/250 [==============================] - 3s 8ms/step - loss: 0.1669 - accuracy: 0.9479 - val_loss: 0.1309 - val_accuracy: 0.9630\n",
      "Epoch 5/10\n",
      "250/250 [==============================] - 3s 9ms/step - loss: 0.1539 - accuracy: 0.9525 - val_loss: 0.1063 - val_accuracy: 0.9655\n",
      "Epoch 6/10\n",
      "250/250 [==============================] - 3s 8ms/step - loss: 0.1088 - accuracy: 0.9688 - val_loss: 0.1049 - val_accuracy: 0.9685\n",
      "Epoch 7/10\n",
      "250/250 [==============================] - 3s 8ms/step - loss: 0.0845 - accuracy: 0.9745 - val_loss: 0.0563 - val_accuracy: 0.9820\n",
      "Epoch 8/10\n",
      "250/250 [==============================] - 3s 8ms/step - loss: 0.0678 - accuracy: 0.9810 - val_loss: 0.0430 - val_accuracy: 0.9890\n",
      "Epoch 9/10\n",
      "250/250 [==============================] - 3s 8ms/step - loss: 0.0363 - accuracy: 0.9900 - val_loss: 0.0144 - val_accuracy: 0.9980\n",
      "Epoch 10/10\n",
      "250/250 [==============================] - 3s 8ms/step - loss: 0.0209 - accuracy: 0.9942 - val_loss: 0.0107 - val_accuracy: 0.9975\n"
     ]
    }
   ],
   "source": [
    "# Example: Training with TensorBoard callback\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=dataset_generator.vocab_size,\n",
    "        output_dim=64,\n",
    "        mask_zero=True\n",
    "    ),\n",
    "    tf.keras.layers.LSTM(64, return_sequences=False),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=10,\n",
    "    callbacks=[tensorboard_callback],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3a5f23",
   "metadata": {},
   "source": [
    "The model performance is excelent. Sequences are not that long and the rules are procedural, there is no need even for a bidirectional encoder decoder architecture that process information in both directions before making an inference. A simple LSTM network is enough to solve this problem"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
