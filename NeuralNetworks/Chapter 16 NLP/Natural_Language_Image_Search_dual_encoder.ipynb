{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJ3tch3DW_Td",
        "outputId": "9d2c7bca-1361-4887-9585-b70059159942"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jax/_src/cloud_tpu_init.py:86: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c \"echo always > /sys/kernel/mm/transparent_hugepage/enabled\")\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import collections\n",
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Suppressing tf.hub warnings\n",
        "tf.get_logger().setLevel(\"ERROR\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "il9TYXI0XC1t",
        "outputId": "f44272c2-6513-432c-fc3a-4cc2363dd824"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.19.1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "root_dir = \"datasets\"\n",
        "# Define the expected location after tf.keras.utils.get_file extracts the zips\n",
        "extracted_captions_root = os.path.join(os.path.abspath(\".\"), root_dir, \"captions_extracted\")\n",
        "annotations_dir = os.path.join(extracted_captions_root, \"captions\", \"annotations\") # Corrected path for annotations\n",
        "\n",
        "extracted_images_root = os.path.join(os.path.abspath(\".\"), root_dir, \"train2014_extracted\")\n",
        "images_dir = os.path.join(extracted_images_root, \"train2014\") # Updated to include the 'train2014' subfolder\n",
        "\n",
        "tfrecords_dir = os.path.join(root_dir, \"tfrecords\")\n",
        "\n",
        "# Ensure target directories exist before extraction\n",
        "tf.io.gfile.makedirs(extracted_captions_root)\n",
        "tf.io.gfile.makedirs(extracted_images_root)\n",
        "\n",
        "# Download caption annotation files if the actual annotations_dir doesn't exist\n",
        "if not os.path.exists(annotations_dir):\n",
        "    tf.keras.utils.get_file(\n",
        "        \"captions.zip\",\n",
        "        cache_dir=extracted_captions_root, # Changed cache_dir to the intended extraction root\n",
        "        origin=\"http://images.cocodataset.org/annotations/annotations_trainval2014.zip\",\n",
        "        extract=True,\n",
        "    )\n",
        "\n",
        "\n",
        "# Download image files if the actual images_dir doesn't exist\n",
        "if not os.path.exists(images_dir): # Changed to check for the actual images_dir\n",
        "    tf.keras.utils.get_file(\n",
        "        \"train2014.zip\",\n",
        "        cache_dir=extracted_images_root, # Changed cache_dir to the intended extraction root\n",
        "        origin=\"http://images.cocodataset.org/zips/train2014.zip\",\n",
        "        extract=True,\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "URKykZsZXaFO",
        "outputId": "b9a48d72-9f6a-4737-c849-0cea809511dc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset is downloaded and extracted successfully.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/datasets/captions_extracted/captions/annotations/captions_train2014.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4031640108.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mannotation_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotations_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"captions_train2014.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotation_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mannotations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"annotations\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/datasets/captions_extracted/captions/annotations/captions_train2014.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(annotations_dir)\n",
        "\n",
        "annotation_file = os.path.join(annotations_dir, \"captions_train2014.json\")\n",
        "\n",
        "with open(annotation_file, \"r\") as f:\n",
        "    annotations = json.load(f)[\"annotations\"]\n",
        "\n",
        "image_path_to_caption = collections.defaultdict(list)\n",
        "for element in annotations:\n",
        "    caption = f\"{element['caption'].lower().rstrip('.')}\"\n",
        "    image_path = os.path.join(images_dir, f\"COCO_train2014_{element['image_id']:012d}.jpg\")\n",
        "    image_path_to_caption[image_path].append(caption)\n",
        "\n",
        "image_paths = list(image_path_to_caption.keys())\n",
        "print(f\"Number of images: {len(image_paths)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Eh1w_KY1dHq",
        "outputId": "08aa5dcb-f39c-441e-bf46-73797052f4c1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/datasets/captions_extracted/captions/annotations\n",
            "Number of images: 82783\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = 30000\n",
        "valid_size = 5000\n",
        "captions_per_image = 2\n",
        "images_per_file = 2000\n",
        "\n",
        "train_image_paths = image_paths[:train_size]\n",
        "num_train_files = int(np.ceil(train_size / images_per_file))\n",
        "train_files_prefix = os.path.join(tfrecords_dir, \"train\")\n",
        "\n",
        "valid_image_paths = image_paths[-valid_size:]\n",
        "num_valid_files = int(np.ceil(valid_size / images_per_file))\n",
        "valid_files_prefix = os.path.join(tfrecords_dir, \"valid\")\n",
        "\n",
        "tf.io.gfile.makedirs(tfrecords_dir)\n",
        "\n",
        "\n",
        "def bytes_feature(value):\n",
        "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
        "\n",
        "\n",
        "def create_example(image_path, caption):\n",
        "    feature = {\n",
        "        \"caption\": bytes_feature(caption.encode()),\n",
        "        \"raw_image\": bytes_feature(tf.io.read_file(image_path).numpy()),\n",
        "    }\n",
        "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "\n",
        "\n",
        "def write_tfrecords(file_name, image_paths):\n",
        "    caption_list = []\n",
        "    image_path_list = []\n",
        "    for image_path in image_paths:\n",
        "        captions = image_path_to_caption[image_path][:captions_per_image]\n",
        "        caption_list.extend(captions)\n",
        "        image_path_list.extend([image_path] * len(captions))\n",
        "\n",
        "    with tf.io.TFRecordWriter(file_name) as writer:\n",
        "        for example_idx in range(len(image_path_list)):\n",
        "            example = create_example(\n",
        "                image_path_list[example_idx], caption_list[example_idx]\n",
        "            )\n",
        "            writer.write(example.SerializeToString())\n",
        "    return example_idx + 1\n",
        "\n",
        "\n",
        "def write_data(image_paths, num_files, files_prefix):\n",
        "    example_counter = 0\n",
        "    for file_idx in tqdm(range(num_files)):\n",
        "        file_name = files_prefix + \"-%02d.tfrecord\" % (file_idx)\n",
        "        start_idx = images_per_file * file_idx\n",
        "        end_idx = start_idx + images_per_file\n",
        "        example_counter += write_tfrecords(file_name, image_paths[start_idx:end_idx])\n",
        "    return example_counter\n",
        "\n",
        "\n",
        "train_example_count = write_data(train_image_paths, num_train_files, train_files_prefix)\n",
        "print(f\"{train_example_count} training examples were written to tfrecord files.\")\n",
        "\n",
        "valid_example_count = write_data(valid_image_paths, num_valid_files, valid_files_prefix)\n",
        "print(f\"{valid_example_count} evaluation examples were written to tfrecord files.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3H6jnBGJZnzt",
        "outputId": "3785a27c-4495-4ec3-8c69-e769d61b5bed"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [09:05<00:00, 36.40s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000 training examples were written to tfrecord files.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [02:20<00:00, 46.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000 evaluation examples were written to tfrecord files.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_description = {\n",
        "    \"caption\": tf.io.FixedLenFeature([], tf.string),\n",
        "    \"raw_image\": tf.io.FixedLenFeature([], tf.string),\n",
        "}\n",
        "\n",
        "\n",
        "def read_example(example):\n",
        "    features = tf.io.parse_single_example(example, feature_description)\n",
        "    raw_image = features.pop(\"raw_image\")\n",
        "    features[\"image\"] = tf.image.resize(\n",
        "        tf.image.decode_jpeg(raw_image, channels=3), size=(299, 299)\n",
        "    )\n",
        "    return features\n",
        "\n",
        "\n",
        "def get_dataset(file_pattern, batch_size):\n",
        "\n",
        "    return (\n",
        "        tf.data.TFRecordDataset(tf.data.Dataset.list_files(file_pattern))\n",
        "        .map(\n",
        "            read_example,\n",
        "            num_parallel_calls=tf.data.AUTOTUNE,\n",
        "            deterministic=False,\n",
        "        )\n",
        "        .shuffle(batch_size * 10)\n",
        "        .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "        .batch(batch_size)\n",
        "    )"
      ],
      "metadata": {
        "id": "ca67KuGonDz9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The projection head is used to transform the image and the text embeddings to the same embedding space with the same dimensionality."
      ],
      "metadata": {
        "id": "GoNgoEFmnK1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def project_embeddings(\n",
        "    embeddings, num_projection_layers, projection_dims, dropout_rate\n",
        "):\n",
        "    projected_embeddings = layers.Dense(units=projection_dims)(embeddings)\n",
        "    for _ in range(num_projection_layers):\n",
        "        x = layers.Activation('gelu')(projected_embeddings) # Changed tf.nn.gelu to layers.Activation('gelu')\n",
        "        x = layers.Dense(projection_dims)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "        x = layers.Add()([projected_embeddings, x])\n",
        "        projected_embeddings = layers.LayerNormalization()(x)\n",
        "    return projected_embeddings"
      ],
      "metadata": {
        "id": "gcggxoG8nGpr"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vision_encoder(\n",
        "    num_projection_layers, projection_dims, dropout_rate, trainable=False\n",
        "):\n",
        "    # Load the pre-trained Xception model to be used as the base encoder.\n",
        "    xception = keras.applications.Xception(\n",
        "        include_top=False, weights=\"imagenet\", pooling=\"avg\"\n",
        "    )\n",
        "    # Set the trainability of the base encoder.\n",
        "    for layer in xception.layers:\n",
        "        layer.trainable = trainable\n",
        "    # Receive the images as inputs.\n",
        "    inputs = layers.Input(shape=(299, 299, 3), name=\"image_input\")\n",
        "    # Preprocess the input image.\n",
        "    xception_input = tf.keras.applications.xception.preprocess_input(inputs)\n",
        "    # Generate the embeddings for the images using the xception model.\n",
        "    embeddings = xception(xception_input)\n",
        "    # Project the embeddings produced by the model.\n",
        "    outputs = project_embeddings(\n",
        "        embeddings, num_projection_layers, projection_dims, dropout_rate\n",
        "    )\n",
        "    # Create the vision encoder model.\n",
        "    return keras.Model(inputs, outputs, name=\"vision_encoder\")"
      ],
      "metadata": {
        "id": "ze04IycJnPEn"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Due to problems with Keras 3 and the BERT model used in the example, i will use BERT from keras-NLP that is compatible"
      ],
      "metadata": {
        "id": "fbPxzoCHuVIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U keras-nlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2N1kGKOuQOi",
        "outputId": "07f9201f-9aa4-46b5-948d-b1453e9145a7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-nlp in /usr/local/lib/python3.12/dist-packages (0.24.0)\n",
            "Requirement already satisfied: keras-hub==0.24.0 in /usr/local/lib/python3.12/dist-packages (from keras-nlp) (0.24.0)\n",
            "Requirement already satisfied: keras>=3.8 in /usr/local/lib/python3.12/dist-packages (from keras-hub==0.24.0->keras-nlp) (3.10.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from keras-hub==0.24.0->keras-nlp) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from keras-hub==0.24.0->keras-nlp) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from keras-hub==0.24.0->keras-nlp) (25.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from keras-hub==0.24.0->keras-nlp) (2025.11.3)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras-hub==0.24.0->keras-nlp) (14.2.0)\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.12/dist-packages (from keras-hub==0.24.0->keras-nlp) (0.3.13)\n",
            "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.12/dist-packages (from keras-hub==0.24.0->keras-nlp) (2.19.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.8->keras-hub==0.24.0->keras-nlp) (0.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from keras>=3.8->keras-hub==0.24.0->keras-nlp) (3.15.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.8->keras-hub==0.24.0->keras-nlp) (0.18.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.12/dist-packages (from keras>=3.8->keras-hub==0.24.0->keras-nlp) (0.5.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from kagglehub->keras-hub==0.24.0->keras-nlp) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kagglehub->keras-hub==0.24.0->keras-nlp) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kagglehub->keras-hub==0.24.0->keras-nlp) (4.67.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras-hub==0.24.0->keras-nlp) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras-hub==0.24.0->keras-nlp) (2.19.2)\n",
            "Requirement already satisfied: tensorflow<2.20,>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow-text->keras-hub==0.24.0->keras-nlp) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras-hub==0.24.0->keras-nlp) (0.1.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub==0.24.0->keras-nlp) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub==0.24.0->keras-nlp) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub==0.24.0->keras-nlp) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub==0.24.0->keras-nlp) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub==0.24.0->keras-nlp) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub==0.24.0->keras-nlp) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub==0.24.0->keras-nlp) (5.29.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub==0.24.0->keras-nlp) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub==0.24.0->keras-nlp) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub==0.24.0->keras-nlp) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub==0.24.0->keras-nlp) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub==0.24.0->keras-nlp) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub==0.24.0->keras-nlp) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub==0.24.0->keras-nlp) (2.19.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub->keras-hub==0.24.0->keras-nlp) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub->keras-hub==0.24.0->keras-nlp) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub->keras-hub==0.24.0->keras-nlp) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub->keras-hub==0.24.0->keras-nlp) (2025.11.12)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub==0.24.0->keras-nlp) (0.45.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub==0.24.0->keras-nlp) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub==0.24.0->keras-nlp) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub==0.24.0->keras-nlp) (3.1.4)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub==0.24.0->keras-nlp) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras_nlp as keras_nlp\n",
        "\n",
        "def create_text_encoder(\n",
        "    num_projection_layers, projection_dims, dropout_rate, trainable=False\n",
        "):\n",
        "    # Usamos un preset de BERT de KerasNLP.\n",
        "    # Podés cambiar a \"bert_tiny_en_uncased\" si querés algo más liviano.\n",
        "    backbone = keras_nlp.models.BertBackbone.from_preset(\n",
        "        \"bert_base_en_uncased\",\n",
        "        load_weights=True,\n",
        "    )\n",
        "    backbone.trainable = trainable\n",
        "\n",
        "    # El preprocessor se encarga de tokenizar y generar las máscaras, etc.\n",
        "    preprocessor = keras_nlp.models.BertPreprocessor.from_preset(\n",
        "        \"bert_base_en_uncased\",\n",
        "        sequence_length=128,  # ajustá el largo según tu caso\n",
        "    )\n",
        "\n",
        "    # Input de texto crudo (igual que antes).\n",
        "    inputs = layers.Input(shape=(), dtype=tf.string, name=\"text_input\")\n",
        "\n",
        "    # Preprocesamiento (tokenización, ids, máscaras…).\n",
        "    bert_inputs = preprocessor(inputs)\n",
        "\n",
        "    # Pasamos por el backbone BERT.\n",
        "    # El backbone devuelve un dict con \"pooled_output\" y \"sequence_output\".\n",
        "    bert_outputs = backbone(bert_inputs)\n",
        "    embeddings = bert_outputs[\"pooled_output\"]  # [batch, hidden_dim]\n",
        "\n",
        "    # Proyectamos a la dimensión que quieras compartir con la rama de imágenes.\n",
        "    outputs = project_embeddings(\n",
        "        embeddings, num_projection_layers, projection_dims, dropout_rate\n",
        "    )\n",
        "\n",
        "    return keras.Model(inputs, outputs, name=\"text_encoder\")"
      ],
      "metadata": {
        "id": "DUjDYqjtufF8"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the dual encoder\n",
        "\n",
        "To calculate the loss, we compute the pairwise dot-product similarity between each caption_i and images_j in the batch as the predictions. The target similarity between caption_i and image_j is computed as the average of the (dot-product similarity between caption_i and caption_j) and (the dot-product similarity between image_i and image_j). Then, we use crossentropy to compute the loss between the targets and the predictions."
      ],
      "metadata": {
        "id": "OBhRQoRCo-MK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DualEncoder(keras.Model):\n",
        "    def __init__(self, text_encoder, image_encoder, temperature=1.0, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.text_encoder = text_encoder\n",
        "        self.image_encoder = image_encoder\n",
        "        self.temperature = temperature\n",
        "        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.loss_tracker]\n",
        "\n",
        "    def call(self, features, training=False):\n",
        "        # Place each encoder on a separate GPU (if available).\n",
        "        # TF will fallback on available devices if there are fewer than 2 GPUs.\n",
        "        with tf.device(\"/gpu:0\"):\n",
        "            # Get the embeddings for the captions.\n",
        "            caption_embeddings = text_encoder(features[\"caption\"], training=training)\n",
        "        with tf.device(\"/gpu:1\"):\n",
        "            # Get the embeddings for the images.\n",
        "            image_embeddings = vision_encoder(features[\"image\"], training=training)\n",
        "        return caption_embeddings, image_embeddings\n",
        "\n",
        "    def compute_loss(self, caption_embeddings, image_embeddings):\n",
        "        # logits[i][j] is the dot_similarity(caption_i, image_j).\n",
        "        logits = (\n",
        "            tf.matmul(caption_embeddings, image_embeddings, transpose_b=True)\n",
        "            / self.temperature\n",
        "        )\n",
        "        # images_similarity[i][j] is the dot_similarity(image_i, image_j).\n",
        "        images_similarity = tf.matmul(\n",
        "            image_embeddings, image_embeddings, transpose_b=True\n",
        "        )\n",
        "        # captions_similarity[i][j] is the dot_similarity(caption_i, caption_j).\n",
        "        captions_similarity = tf.matmul(\n",
        "            caption_embeddings, caption_embeddings, transpose_b=True\n",
        "        )\n",
        "        # targets[i][j] = avarage dot_similarity(caption_i, caption_j) and dot_similarity(image_i, image_j).\n",
        "        targets = keras.activations.softmax(\n",
        "            (captions_similarity + images_similarity) / (2 * self.temperature)\n",
        "        )\n",
        "        # Compute the loss for the captions using crossentropy\n",
        "        captions_loss = keras.losses.categorical_crossentropy(\n",
        "            y_true=targets, y_pred=logits, from_logits=True\n",
        "        )\n",
        "        # Compute the loss for the images using crossentropy\n",
        "        images_loss = keras.losses.categorical_crossentropy(\n",
        "            y_true=tf.transpose(targets), y_pred=tf.transpose(logits), from_logits=True\n",
        "        )\n",
        "        # Return the mean of the loss over the batch.\n",
        "        return (captions_loss + images_loss) / 2\n",
        "\n",
        "    def train_step(self, features):\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Forward pass\n",
        "            caption_embeddings, image_embeddings = self(features, training=True)\n",
        "            loss = self.compute_loss(caption_embeddings, image_embeddings)\n",
        "        # Backward pass\n",
        "        gradients = tape.gradient(loss, self.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "        # Monitor loss\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        return {\"loss\": self.loss_tracker.result()}\n",
        "\n",
        "    def test_step(self, features):\n",
        "        caption_embeddings, image_embeddings = self(features, training=False)\n",
        "        loss = self.compute_loss(caption_embeddings, image_embeddings)\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        return {\"loss\": self.loss_tracker.result()}"
      ],
      "metadata": {
        "id": "gtnLD-IppD9o"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 5  # In practice, train for at least 30 epochs\n",
        "batch_size = 256\n",
        "\n",
        "vision_encoder = create_vision_encoder(\n",
        "    num_projection_layers=1, projection_dims=256, dropout_rate=0.1\n",
        ")\n",
        "text_encoder = create_text_encoder(\n",
        "    num_projection_layers=1, projection_dims=256, dropout_rate=0.1\n",
        ")\n",
        "dual_encoder = DualEncoder(text_encoder, vision_encoder, temperature=0.05)\n",
        "dual_encoder.compile(\n",
        "    optimizer=tf.optimizers.AdamW(learning_rate=0.001, weight_decay=0.001)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MugwM_j9p0A6",
        "outputId": "ab80a877-3205-477d-9ae1-4225bbe78645"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m83683744/83683744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of GPUs: {len(tf.config.list_physical_devices('GPU'))}\")\n",
        "print(f\"Number of examples (caption-image pairs): {train_example_count}\")\n",
        "print(f\"Batch size: {batch_size}\")\n",
        "print(f\"Steps per epoch: {int(np.ceil(train_example_count / batch_size))}\")\n",
        "train_dataset = get_dataset(os.path.join(tfrecords_dir, \"train-*.tfrecord\"), batch_size)\n",
        "valid_dataset = get_dataset(os.path.join(tfrecords_dir, \"valid-*.tfrecord\"), batch_size)\n",
        "# Create a learning rate scheduler callback.\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor=\"val_loss\", factor=0.2, patience=3\n",
        ")\n",
        "# Create an early stopping callback.\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\", patience=5, restore_best_weights=True\n",
        ")\n",
        "history = dual_encoder.fit(\n",
        "    train_dataset,\n",
        "    epochs=num_epochs,\n",
        "    validation_data=valid_dataset,\n",
        "    callbacks=[reduce_lr, early_stopping],\n",
        ")\n",
        "print(\"Training completed. Saving vision and text encoders...\")\n",
        "vision_encoder.save(\"vision_encoder\")\n",
        "text_encoder.save(\"text_encoder\")\n",
        "print(\"Models are saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VaUTRV-xvWZq",
        "outputId": "4590be2a-dc69-4999-b248-f7b3d097088d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of GPUs: 0\n",
            "Number of examples (caption-image pairs): 60000\n",
            "Batch size: 256\n",
            "Steps per epoch: 235\n",
            "Epoch 1/5\n",
            "      5/Unknown \u001b[1m634s\u001b[0m 126s/step - loss: 519.3160"
          ]
        }
      ]
    }
  ]
}