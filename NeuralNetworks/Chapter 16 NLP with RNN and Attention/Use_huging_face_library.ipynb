{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7546c5d2",
   "metadata": {},
   "source": [
    "# Shakespearean Text Generation with Hugging Face Transformers\n",
    "\n",
    "This notebook shows how to load a pretrained causal language model from the Hugging Face Hub and use its `generate()` method to craft text that mimics Shakespeare's diction. Feel free to adapt the prompts and decoding parameters to experiment with different styles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d05ec37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet transformers torch accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7d85f8",
   "metadata": {},
   "source": [
    "## Load a lightweight Shakespeare-friendly model\n",
    "\n",
    "The `distilgpt2` checkpoint is a distilled version of GPT-2 that fits on most laptops while still producing coherent prose. For better results you can swap in larger checkpoints (e.g., `gpt2-medium`, `tiiuae/falcon-7b-instruct`) if your hardware allows it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "747ae10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "842921ab29e541a2ab0c8ab4a4f716b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef662939b35940a5881357d03b9307b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:  92%|#########1| 9.13G/9.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "660b997383da4cb89a41579f1bfee7b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.48G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab49ad2d34ae4ca7bea4a789a909006f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cd02b77a120419f88565b89364b2133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/117 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "MODEL_NAME = \"tiiuae/falcon-7b-instruct\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {DEVICE}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "model.to(DEVICE)\n",
    "model.eval();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358a2db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_shakespearean_text(\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 120,\n",
    "    temperature: float = 0.95,\n",
    "    top_p: float = 0.92,\n",
    "    repetition_penalty: float = 1.05,\n",
    "    seed: int | None = 42,\n",
    ") -> str:\n",
    "    \"\"\"Generate Shakespearean-style text using nucleus sampling.\"\"\"\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9e38a0",
   "metadata": {},
   "source": [
    "## Try a few Shakespearean prompts\n",
    "\n",
    "Adjust the prompt and decoding hyperparameters to steer tone, length, and creativity. Lower `temperature` or `top_p` for more conservative prose; raise them for adventurous language.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ecf72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a Shakespearean sonnet with 14 lines and iambic pentameter:\n",
      "\n",
      "He sings at three-o'clock, to see what's out of place\n",
      ".. The King is dead, that he might live; He must not go through town or country Again To write this poem, then again It'll be very interesting if I were able to translate it for you All the times since Henry IV died When would his grief have filled me? If in your time there was no hope but God That any good may come back to him Who could endure so long an age on earth So much had life lived, such a lot had wroth And still we look upon nothing new as fresh\n"
     ]
    }
   ],
   "source": [
    "prompt = (\n",
    "    \"Write a Shakespearean sonnet with 14 lines and iambic pentameter\"\n",
    ")\n",
    "\n",
    "sample = generate_shakespearean_text(\n",
    "    prompt,\n",
    "    max_new_tokens=125,\n",
    "    temperature=1.2,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.03,\n",
    ")\n",
    "print(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f73dfd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
