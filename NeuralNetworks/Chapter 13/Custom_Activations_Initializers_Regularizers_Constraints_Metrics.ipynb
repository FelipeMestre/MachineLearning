{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dfc3401-fa76-4e22-a1f1-6cc7c1622f82",
   "metadata": {},
   "source": [
    "# Personalización de Activation Functions, Initializers, Regularizers, and Constraints\n",
    "\n",
    "En general el proceso es el mismo que el indicado en la sección de pérdidas. Se crea una función que toma los parámetros necesarios y dentro se ejecuta la lógica personalizada que se quiera, devolviendo un valor del tipo necesario. O es posible también generar una subclase pertinente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4745867b-9efc-4d03-846c-a87a87966d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "def my_softplus(z):\n",
    "    return tf.math.log(1.0 + tf.exp(z))\n",
    "\n",
    "def my_glorot_initializer(shape, dtype=tf.float32):\n",
    "    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\n",
    "    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n",
    "\n",
    "def my_l1_regularizer(weights):\n",
    "    return tf.reduce_sum(tf.abs(0.01 * weights))\n",
    "\n",
    "def my_positive_weights(weights):\n",
    "    return tf.where(weights < 0., tf.zeros_like(weights), weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "360196e8-3776-48e1-80a8-15b7887c0e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = tf.keras.layers.Dense(1, activation=my_softplus,\n",
    "                              kernel_initializer=my_glorot_initializer,\n",
    "                              kernel_regularizer=my_l1_regularizer,\n",
    "                              kernel_constraint=my_positive_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0292897-e132-4093-a692-1d16abb9733a",
   "metadata": {},
   "source": [
    "1. La función activación se aplicará al output de esta capa densa y el resultado será input de la siguiente capa\n",
    "2. Los pesos de la capa van a ser inicializados usando el valor retornado por el inicializador\n",
    "3. En cada paso del entrenamiento los pesos van a pasar por la regularización para computar la pérdida por regularización, que van a ser sumados al peso principal para obtener la pérdida final usada para entrenar\n",
    "4. El último paso, la función constraint va a ser llamada después de cada paso de entrenamiento, y los pesos de la capa van a ser reemplazados por los pesos constrained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edb4d61-9c87-49b5-91fe-882f2cd226be",
   "metadata": {},
   "source": [
    "Si se desean guardar los hyperparametros de una función con el modelo es necesario implementar una subclase de la entidad necesaria como\n",
    "- tf.keras.regularizers.Regularizer\n",
    "- tf.keras.constraints.Constraint\n",
    "- tf.keras.optimizers.Optimizer\n",
    "- tf.keras.initializers.Initializer\n",
    "- tf.keras.layers.Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "720909fd-0c2d-4a4a-b78d-b19376b1e4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyL1Regularizer(tf.keras.regularizers.Regularizer):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "    \n",
    "    def __call__(self, weights):\n",
    "        return tf.reduce_sum(tf.abs(self.factor * weights))\n",
    "        \n",
    "    def get_config(self):\n",
    "        return {\"factor\": self.factor} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4bba68-5173-4845-9b89-af9932aede67",
   "metadata": {},
   "outputs": [],
   "source": [
    "Note that you must implement the call() method for losses, layers (including\n",
    "activation functions), and models, or the\n",
    "call\n",
    "__\n",
    "__() method for regularizers,\n",
    "initializers, and constraints. For metrics, things are a bit different, as you will\n",
    "see now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b661c98-0a77-417e-91ce-9f9fccf726c7",
   "metadata": {},
   "source": [
    "Para pérdidas, capaz y modelos se usa call(). Para regularizadores, intializadores o constraints __call__()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9381b3-0f58-4d19-baf0-d6a773c03153",
   "metadata": {},
   "source": [
    "### Métricas\n",
    "Una métrica difiere de la pérdida en cuanto la función pérdida necesita ser derivable y distinta de cero en el dominio correspondiente, además de que no es necesario que sea fácilmente interpretable con quien la lee. Las métricas por el contrario deben tender a ser fáciles de interpretar\n",
    "\n",
    "Definir una métrica personalizada es bastante parecido a definir una función pérdida, para ello utilizaré la misma función huber loss, usandóla como métrica\n",
    "\n",
    "Para cada lote de entrenamiento Keras computa la función y lleva rastro de su promedio a lo largo de la época. A veces no se requiere que lleve el promedio, como es el caso de la precisión de clasificación binaria, porque es el cociente entre las predicciones acertadas de una clase y la suma entre las predicciones erradas y las acertadas, por lo tanto, acumular el promedio no refleja el valor de la métrica real. Estas mediciones que se actualizan de batch a batch se llaman streaming metrics. Lo que se hace es una clase que conozca los valores de predicciónes positivas y negativas y actualice esta métrica como tf.keras.metric.Precision. En cualquier punto de la ejecución se puede llamar al método result() para obtener el valor de la métrica o ver sus variables con el atributo variables y se pueden resetear con el método reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31c1a417-1bbe-4780-9c5b-d38d153c75dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "@tf.function\n",
    "def huber_fn(y_true, y_pred, delta=1.0):\n",
    "    error = y_true - y_pred\n",
    "    abs_error = tf.abs(error)\n",
    "    quadratic = 0.5 * tf.square(error)\n",
    "    linear = delta * (abs_error - 0.5 * delta)\n",
    "    return tf.where(abs_error <= delta, quadratic, linear)\n",
    "\n",
    "class HuberMetric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.huber_fn = create_huber(threshold)\n",
    "        self.total = self.add_weight(\"total\", initializer=\"zeros\")\n",
    "        self.count = self.add_weight(\"count\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        sample_metrics = self.huber_fn(y_true, y_pred)\n",
    "        self.total.assign_add(tf.reduce_sum(sample_metrics))\n",
    "        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return self.total / self.count\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5fab00-3161-4493-ab7f-4cfd718e9adb",
   "metadata": {},
   "source": [
    "- El constructor usa add_weight para crear las variables necesarias en cero. Se podrían usar variables normales, pero keras mantiene tarzabilidad de los tf.variables o más generalmente de los trackable\n",
    "- update_state, calcula la función huber_fn, agrega el valor a la varaible total y le suma la cantidad de las predicciones a la variable count.\n",
    "- result() calcula la métrica objetivo sobre todas las instancias. Keras llama siempre a update_state primero y después a result\n",
    "- get_config está para que se guarde el threshold si se guarda el modelo\n",
    "\n",
    "Cuando se define una métrica usando una función Keras automáticamente la llama para cada lote, y lleva rastro de la métrica para cada época. El único beneficio de esta clase es para guardar parámetros al guardar el modelo y que algunas métricas como la precisión no se realizan simplemente promediando entre batches."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
