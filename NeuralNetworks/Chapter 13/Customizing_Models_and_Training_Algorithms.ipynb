{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5504a33-ab64-43b2-b071-635a73420427",
   "metadata": {},
   "source": [
    "# Customizing Models and training algorithms\n",
    "\n",
    "## Loss Function\n",
    "En algunos casos quizás por las características de los datos, sea necesario utilizar funciones de costo alternativas, como en este ejemplo donde se simula la implementación de Huber loss function a pesar de que keras contiene tf.keras.losses.Huber. \n",
    "\n",
    "En un caso hipotético donde se entrena un modelo de regresión, y luego de realizar la limpieza de outliers de los datos, aún mantienen cierto ruido, por lo cual el error cuadratico medio puede penalizar demasiado a errores largos y generar un modelo impreciso. El error absoluto medio quizás no penaliza tanto a los outliers, pero el entrenamiento pueden demorar en converger, y el modelo entrenado puede ser impreciso también. Por lo tanto se procede a usar una función de pérdida custom en lugar de MSE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f92f0cd-7653-4c80-8805-5fc2d7d0e698",
   "metadata": {},
   "source": [
    "```Python\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "def huber_fn(y_true, y_pred):\n",
    "    error = y_true - y_pred\n",
    "    max_error = 1\n",
    "    is_small_error = tf.abs(error) < max_error\n",
    "    squared_loss = tf.square(error)\n",
    "    linear_loss = tf.abs(error) - 0.5\n",
    "    return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa1f281-5411-4f94-9f2f-0048f6c336cb",
   "metadata": {},
   "source": [
    "Para mejor performance se debe utilizar únicamente una implementación vectorial, para beneficiarse de la optimización de TensorFlow solo se deben usar operaciones de TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4146f701-9e24-41c5-955f-ba58cb66c8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "@tf.function\n",
    "def huber_fn(y_true, y_pred, delta=1.0):\n",
    "    error = y_true - y_pred\n",
    "    abs_error = tf.abs(error)\n",
    "    quadratic = 0.5 * tf.square(error)\n",
    "    linear = delta * (abs_error - 0.5 * delta)\n",
    "    return tf.where(abs_error <= delta, quadratic, linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735579a0-eb8a-4238-9de1-f7c303d0b877",
   "metadata": {},
   "source": [
    "- Vectorización total: todo opera sobre tensores, sin bucles.\n",
    "- Multiplica por delta en la parte lineal (más general y matemáticamente correcto).\n",
    "- Decorador @tf.function: compila la función en una graph function, mejorando performance al ser usado en entrenamiento o producción."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98360f38-3436-47d4-a4ed-1fdcf38394ed",
   "metadata": {},
   "source": [
    "Se utilizará el dataset de Housing para simular un problema de regresión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fc9541a-a480-41a1-b59d-8f494bf52b92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404, 13)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import boston_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = boston_housing.load_data()\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568222c0-ec5e-4c00-ae0f-369af5dcdb50",
   "metadata": {},
   "source": [
    "404 muestras con 13 atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "182d7de9-9df5-4874-a3e6-82c2552a8a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 22.4025 - mae: 22.9025 - mse: 610.4813 - val_loss: 22.8437 - val_mae: 23.3437 - val_mse: 629.6715\n",
      "Epoch 2/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 22.2264 - mae: 22.7264 - mse: 602.6571 - val_loss: 22.7012 - val_mae: 23.2012 - val_mse: 622.4734\n",
      "Epoch 3/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 22.0687 - mae: 22.5687 - mse: 595.4377 - val_loss: 22.5678 - val_mae: 23.0678 - val_mse: 615.8667\n",
      "Epoch 4/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 21.9249 - mae: 22.4249 - mse: 588.6517 - val_loss: 22.4375 - val_mae: 22.9375 - val_mse: 609.5627\n",
      "Epoch 5/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 21.7781 - mae: 22.2781 - mse: 582.2001 - val_loss: 22.3055 - val_mae: 22.8055 - val_mse: 603.3384\n",
      "Epoch 6/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 21.6386 - mae: 22.1386 - mse: 575.9510 - val_loss: 22.1678 - val_mae: 22.6678 - val_mse: 597.0446\n",
      "Epoch 7/200\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 20.9431 - mae: 21.4431 - mse: 509.3153"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/homl3/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 3ms/step - loss: 21.5349 - mae: 22.0349 - mse: 570.9202 - val_loss: 22.0222 - val_mae: 22.5222 - val_mse: 590.4993\n",
      "Epoch 8/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 21.3652 - mae: 21.8652 - mse: 563.2833 - val_loss: 21.8597 - val_mae: 22.3597 - val_mse: 583.3077\n",
      "Epoch 9/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 21.1375 - mae: 21.6375 - mse: 553.4991 - val_loss: 21.6743 - val_mae: 22.1743 - val_mse: 575.2074\n",
      "Epoch 10/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 20.9574 - mae: 21.4574 - mse: 546.6489 - val_loss: 21.4606 - val_mae: 21.9606 - val_mse: 566.0571\n",
      "Epoch 11/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 20.7393 - mae: 21.2393 - mse: 538.5350 - val_loss: 21.2081 - val_mae: 21.7081 - val_mse: 555.3569\n",
      "Epoch 12/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 20.5376 - mae: 21.0376 - mse: 530.4330 - val_loss: 20.9207 - val_mae: 21.4207 - val_mse: 543.2641\n",
      "Epoch 13/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 20.1973 - mae: 20.6973 - mse: 515.3002 - val_loss: 20.5968 - val_mae: 21.0968 - val_mse: 529.7537\n",
      "Epoch 14/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 19.9125 - mae: 20.4124 - mse: 506.2917 - val_loss: 20.2322 - val_mae: 20.7322 - val_mse: 514.8889\n",
      "Epoch 15/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 19.4429 - mae: 19.9428 - mse: 485.3387 - val_loss: 19.8171 - val_mae: 20.3171 - val_mse: 498.0864\n",
      "Epoch 16/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 18.9976 - mae: 19.4964 - mse: 471.2813 - val_loss: 19.3343 - val_mae: 19.8329 - val_mse: 479.1252\n",
      "Epoch 17/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 18.4883 - mae: 18.9877 - mse: 448.2316 - val_loss: 18.7973 - val_mae: 19.2939 - val_mse: 457.9807\n",
      "Epoch 18/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 18.0951 - mae: 18.5938 - mse: 433.7151 - val_loss: 18.2077 - val_mae: 18.7047 - val_mse: 434.9329\n",
      "Epoch 19/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 17.4014 - mae: 17.9001 - mse: 407.3174 - val_loss: 17.5678 - val_mae: 18.0647 - val_mse: 409.5541\n",
      "Epoch 20/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 16.8231 - mae: 17.3213 - mse: 387.8273 - val_loss: 16.8665 - val_mae: 17.3598 - val_mse: 382.0677\n",
      "Epoch 21/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 16.0136 - mae: 16.5084 - mse: 359.3125 - val_loss: 16.1035 - val_mae: 16.5971 - val_mse: 352.4194\n",
      "Epoch 22/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 15.6145 - mae: 16.1103 - mse: 339.6678 - val_loss: 15.2932 - val_mae: 15.7863 - val_mse: 321.1594\n",
      "Epoch 23/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 14.5230 - mae: 15.0196 - mse: 308.1158 - val_loss: 14.4076 - val_mae: 14.9031 - val_mse: 288.2783\n",
      "Epoch 24/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 13.6101 - mae: 14.1053 - mse: 282.2237 - val_loss: 13.4141 - val_mae: 13.9128 - val_mse: 253.7736\n",
      "Epoch 25/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 12.8108 - mae: 13.3037 - mse: 256.2233 - val_loss: 12.3735 - val_mae: 12.8686 - val_mse: 220.6755\n",
      "Epoch 26/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 12.0611 - mae: 12.5555 - mse: 238.2551 - val_loss: 11.2717 - val_mae: 11.7623 - val_mse: 189.1248\n",
      "Epoch 27/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 11.0363 - mae: 11.5285 - mse: 205.9084 - val_loss: 10.2769 - val_mae: 10.7727 - val_mse: 162.2775\n",
      "Epoch 28/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 10.0208 - mae: 10.5100 - mse: 177.8928 - val_loss: 9.2882 - val_mae: 9.7830 - val_mse: 137.2489\n",
      "Epoch 29/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 9.6758 - mae: 10.1649 - mse: 165.1475 - val_loss: 8.5000 - val_mae: 8.9947 - val_mse: 118.3611\n",
      "Epoch 30/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 9.0469 - mae: 9.5392 - mse: 148.3552 - val_loss: 7.8678 - val_mae: 8.3566 - val_mse: 104.3053\n",
      "Epoch 31/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 8.5346 - mae: 9.0248 - mse: 137.4341 - val_loss: 7.3811 - val_mae: 7.8684 - val_mse: 94.3401\n",
      "Epoch 32/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 8.6114 - mae: 9.0996 - mse: 140.3679 - val_loss: 6.9258 - val_mae: 7.4111 - val_mse: 85.6871\n",
      "Epoch 33/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 8.1916 - mae: 8.6784 - mse: 125.1736 - val_loss: 6.5604 - val_mae: 7.0484 - val_mse: 78.9787\n",
      "Epoch 34/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 8.1758 - mae: 8.6613 - mse: 126.7324 - val_loss: 6.3347 - val_mae: 6.8206 - val_mse: 74.5218\n",
      "Epoch 35/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 8.2652 - mae: 8.7534 - mse: 136.2802 - val_loss: 6.0886 - val_mae: 6.5696 - val_mse: 69.9454\n",
      "Epoch 36/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 7.3906 - mae: 7.8809 - mse: 104.7902 - val_loss: 5.8975 - val_mae: 6.3805 - val_mse: 66.3914\n",
      "Epoch 37/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 7.8486 - mae: 8.3338 - mse: 124.5540 - val_loss: 5.6913 - val_mae: 6.1712 - val_mse: 62.7773\n",
      "Epoch 38/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 7.7217 - mae: 8.2117 - mse: 115.3707 - val_loss: 5.5160 - val_mae: 5.9979 - val_mse: 59.9120\n",
      "Epoch 39/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 6.9097 - mae: 7.3927 - mse: 95.4517 - val_loss: 5.2676 - val_mae: 5.7480 - val_mse: 55.7659\n",
      "Epoch 40/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 7.2824 - mae: 7.7641 - mse: 105.5919 - val_loss: 5.1184 - val_mae: 5.5983 - val_mse: 53.2184\n",
      "Epoch 41/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 6.9659 - mae: 7.4466 - mse: 98.4248 - val_loss: 4.9661 - val_mae: 5.4506 - val_mse: 50.6472\n",
      "Epoch 42/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 7.3753 - mae: 7.8585 - mse: 110.4908 - val_loss: 4.8233 - val_mae: 5.3064 - val_mse: 48.2187\n",
      "Epoch 43/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 6.8800 - mae: 7.3623 - mse: 93.3236 - val_loss: 4.7159 - val_mae: 5.1997 - val_mse: 46.3605\n",
      "Epoch 44/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 7.4135 - mae: 7.8998 - mse: 109.9192 - val_loss: 4.6982 - val_mae: 5.1769 - val_mse: 46.1484\n",
      "Epoch 45/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 6.7172 - mae: 7.2074 - mse: 89.1152 - val_loss: 4.6873 - val_mae: 5.1550 - val_mse: 46.0586\n",
      "Epoch 46/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 6.6329 - mae: 7.1173 - mse: 89.6099 - val_loss: 4.5911 - val_mae: 5.0585 - val_mse: 44.4132\n",
      "Epoch 47/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 7.1956 - mae: 7.6858 - mse: 103.3044 - val_loss: 4.5850 - val_mae: 5.0570 - val_mse: 44.3920\n",
      "Epoch 48/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 7.4775 - mae: 7.9671 - mse: 102.9994 - val_loss: 4.5027 - val_mae: 4.9736 - val_mse: 43.2224\n",
      "Epoch 49/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 6.6854 - mae: 7.1714 - mse: 87.1236 - val_loss: 4.3648 - val_mae: 4.8340 - val_mse: 40.9575\n",
      "Epoch 50/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 7.0336 - mae: 7.5156 - mse: 98.8168 - val_loss: 4.2733 - val_mae: 4.7416 - val_mse: 39.3304\n",
      "Epoch 51/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 6.5957 - mae: 7.0844 - mse: 89.1241 - val_loss: 4.2011 - val_mae: 4.6662 - val_mse: 38.1313\n",
      "Epoch 52/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 6.6733 - mae: 7.1591 - mse: 87.2786 - val_loss: 4.1856 - val_mae: 4.6524 - val_mse: 37.9105\n",
      "Epoch 53/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 7.0982 - mae: 7.5848 - mse: 106.6123 - val_loss: 4.1673 - val_mae: 4.6408 - val_mse: 37.5938\n",
      "Epoch 54/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 7.0014 - mae: 7.4880 - mse: 92.6387 - val_loss: 4.1565 - val_mae: 4.6300 - val_mse: 37.4581\n",
      "Epoch 55/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 6.4499 - mae: 6.9342 - mse: 88.4627 - val_loss: 4.0912 - val_mae: 4.5656 - val_mse: 36.3962\n",
      "Epoch 56/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 7.2886 - mae: 7.7684 - mse: 104.0282 - val_loss: 4.0785 - val_mae: 4.5510 - val_mse: 36.1976\n",
      "Epoch 57/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 6.4105 - mae: 6.8982 - mse: 85.8477 - val_loss: 4.0670 - val_mae: 4.5392 - val_mse: 35.9127\n",
      "Epoch 58/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 6.6123 - mae: 7.0960 - mse: 90.7815 - val_loss: 4.1108 - val_mae: 4.5817 - val_mse: 36.5630\n",
      "Epoch 59/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 7.0783 - mae: 7.5679 - mse: 97.7633 - val_loss: 4.0851 - val_mae: 4.5539 - val_mse: 36.1442\n",
      "Epoch 60/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 6.3609 - mae: 6.8469 - mse: 78.8294 - val_loss: 4.0293 - val_mae: 4.4986 - val_mse: 35.3061\n",
      "Epoch 61/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 6.4239 - mae: 6.9087 - mse: 86.4869 - val_loss: 3.9037 - val_mae: 4.3705 - val_mse: 33.4427\n",
      "Epoch 62/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 7.0298 - mae: 7.5184 - mse: 95.3975 - val_loss: 3.8059 - val_mae: 4.2758 - val_mse: 31.8257\n",
      "Epoch 63/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 6.1794 - mae: 6.6684 - mse: 77.3973 - val_loss: 3.7620 - val_mae: 4.2306 - val_mse: 31.1170\n",
      "Epoch 64/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 6.6979 - mae: 7.1803 - mse: 96.6456 - val_loss: 3.7488 - val_mae: 4.2160 - val_mse: 31.0025\n",
      "Epoch 65/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 6.6532 - mae: 7.1349 - mse: 90.4546 - val_loss: 3.7804 - val_mae: 4.2462 - val_mse: 31.5587\n",
      "Epoch 66/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 5.8504 - mae: 6.3398 - mse: 66.7575 - val_loss: 3.8505 - val_mae: 4.3158 - val_mse: 32.6677\n",
      "Epoch 67/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 6.2405 - mae: 6.7288 - mse: 79.6128 - val_loss: 3.8741 - val_mae: 4.3414 - val_mse: 33.0099\n",
      "Epoch 68/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 6.3633 - mae: 6.8466 - mse: 83.0602 - val_loss: 3.8588 - val_mae: 4.3256 - val_mse: 32.6835\n",
      "Epoch 69/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 6.2696 - mae: 6.7494 - mse: 79.1503 - val_loss: 3.7946 - val_mae: 4.2571 - val_mse: 31.7052\n",
      "Epoch 70/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 6.3100 - mae: 6.7911 - mse: 78.5734 - val_loss: 3.8266 - val_mae: 4.2919 - val_mse: 32.2128\n",
      "Epoch 71/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 5.7080 - mae: 6.1891 - mse: 67.2866 - val_loss: 3.7728 - val_mae: 4.2346 - val_mse: 31.4783\n",
      "Epoch 72/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 6.2093 - mae: 6.6945 - mse: 80.7051 - val_loss: 3.7130 - val_mae: 4.1760 - val_mse: 30.6416\n",
      "Epoch 73/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 5.8419 - mae: 6.3230 - mse: 70.6936 - val_loss: 3.7784 - val_mae: 4.2392 - val_mse: 31.5325\n",
      "Epoch 74/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 6.2115 - mae: 6.6953 - mse: 81.4678 - val_loss: 3.8095 - val_mae: 4.2705 - val_mse: 31.8098\n",
      "Epoch 75/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 5.9494 - mae: 6.4331 - mse: 73.3950 - val_loss: 3.8245 - val_mae: 4.2843 - val_mse: 31.9475\n",
      "Epoch 76/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 6.3939 - mae: 6.8828 - mse: 81.8600 - val_loss: 3.7907 - val_mae: 4.2497 - val_mse: 31.5012\n",
      "Epoch 77/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 6.2172 - mae: 6.6958 - mse: 84.1071 - val_loss: 3.6691 - val_mae: 4.1324 - val_mse: 29.8111\n",
      "Epoch 78/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 6.3068 - mae: 6.7903 - mse: 84.9907 - val_loss: 3.5909 - val_mae: 4.0581 - val_mse: 28.7666\n",
      "Epoch 79/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 6.2474 - mae: 6.7291 - mse: 82.2655 - val_loss: 3.6357 - val_mae: 4.0961 - val_mse: 29.4557\n",
      "Epoch 80/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 6.1673 - mae: 6.6556 - mse: 79.5292 - val_loss: 3.6911 - val_mae: 4.1503 - val_mse: 30.1892\n",
      "Epoch 81/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 6.2161 - mae: 6.6966 - mse: 80.4317 - val_loss: 3.8220 - val_mae: 4.2787 - val_mse: 31.7338\n",
      "Epoch 82/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 6.0631 - mae: 6.5446 - mse: 72.9134 - val_loss: 3.8460 - val_mae: 4.3019 - val_mse: 31.9863\n",
      "Epoch 83/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 6.1893 - mae: 6.6701 - mse: 75.1958 - val_loss: 3.7957 - val_mae: 4.2513 - val_mse: 31.3614\n",
      "Epoch 84/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 5.9384 - mae: 6.4169 - mse: 75.8284 - val_loss: 3.7603 - val_mae: 4.2159 - val_mse: 30.9348\n",
      "Epoch 85/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 5.8159 - mae: 6.2980 - mse: 77.1995 - val_loss: 3.7485 - val_mae: 4.2038 - val_mse: 30.7611\n",
      "Epoch 86/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 6.1914 - mae: 6.6727 - mse: 79.3366 - val_loss: 3.7458 - val_mae: 4.2010 - val_mse: 30.6291\n",
      "Epoch 87/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 6.1951 - mae: 6.6786 - mse: 81.5162 - val_loss: 3.7625 - val_mae: 4.2183 - val_mse: 30.7553\n",
      "Epoch 88/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 5.9003 - mae: 6.3816 - mse: 71.2527 - val_loss: 3.6170 - val_mae: 4.0754 - val_mse: 29.0291\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x16ac3b550>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Dense(32, activation=\"relu\", input_shape=(x_train.shape[1],)),\n",
    "        keras.layers.Dropout(0.4),\n",
    "        keras.layers.Dense(16, activation=\"relu\"),\n",
    "        keras.layers.Dropout(0.3),\n",
    "        keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.0005)\n",
    "\n",
    "model.compile(\n",
    "    loss=huber_fn, \n",
    "    optimizer=optimizer,\n",
    "    metrics=[\"mae\", \"mse\"]  # Métricas apropiadas para regresión, mean absolute error y mean squared error\n",
    "    #metrics=[\"accuracy\"] Aqui se uso accuracy, pero es para clasificación, en regresión se usa mae o mse\n",
    ")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"custom_model_loss_function.h5\", save_best_only=True)\n",
    "\n",
    "model.fit(\n",
    "    x_train, \n",
    "    y_train, \n",
    "    epochs=200, \n",
    "    validation_data=(x_test, y_test),\n",
    "    callbacks=[checkpoint_cb, early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833536ab-273d-4dcb-b2b7-6bf8db6da975",
   "metadata": {},
   "source": [
    "### Saving and loading models containing custom components\n",
    "\n",
    "Con una función costo hay que proveer un diccionario que mapea el nombre de la función a la función en si cuando cargás la misma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ac94c08-2515-4554-8bf1-3def28411a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(\"custom_model_loss_function.h5\",\n",
    "                            custom_objects={\"huber_fn\": huber_fn})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d249de-ba73-4378-a6f7-ae73caeb5036",
   "metadata": {},
   "source": [
    "Con esta implementación, cualquier error entre -1 y 1 es considerado chico, Pero como configurar un treshold distinto? Una solución es modificar la función custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2448fd73-f263-424c-8bd2-30e941cd5b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_huber(treshold=1.0): #Implementacion distinta de la primera, a modo de ejemplo\n",
    "    def huber_fn(y_true, y_predicted):\n",
    "        error = y_true - y_pred\n",
    "        abs_error = tf.abs(error)\n",
    "        quadratic = 0.5 * tf.square(error)\n",
    "        linear = treshold * (abs_error - 0.5 * delta)\n",
    "        return tf.where(abs_error <= treshold, quadratic, linear)\n",
    "    return huber_fn\n",
    "\n",
    "model.compile(loss=create_huber(2.0), optimizer=\"nadam\")\n",
    "model.save(\"custom_model_loss_function_2.h5\")\n",
    "\n",
    "model = tf.keras.models.load_model(\"custom_model_loss_function_2.h5\",\n",
    "                                  custom_objects={\"huber_fn\": create_huber(2.0)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26440bd7-ef03-4329-8476-aa92bca60043",
   "metadata": {},
   "source": [
    "Cuando se guarda el modelo, el umbral no se va a guardar, esto significa que hay que especificarlo cuando se carga el modelo. También es notable que el nombre de funcion que se le da a keras es huber_fn, no create_huber\n",
    "\n",
    "Esto se puede resolver creando una subclase de una clase tf.keras.losses.Loss e implementando su método get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4459b89-2c2f-4c1a-adad-bc7ffb7d244a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuberLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        self.threshold = threshold\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < self.threshold\n",
    "        abs_error = tf.abs(error)\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss = self.threshold * abs_error - self.threshold**2/2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}\n",
    "\n",
    "\n",
    "model.compile(loss=HuberLoss(2.), optimizer=\"nadam\")\n",
    "model.save(\"custom_class_loss_function.h5\")\n",
    "\n",
    "model = tf.keras.models.load_model(\"custom_class_loss_function.h5\",\n",
    "                                   custom_objects={\"HuberLoss\": HuberLoss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006cccc5-189d-426b-a522-bb5adc1e93ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2443a45-6d6c-4394-bd12-812a74c5506d",
   "metadata": {},
   "source": [
    "El constructor acepta **kwargs y lo pasa al padre cuando hace super(**kwargs), esto es para gestionar los hyperparámetros standard, como el nombre de la perdida y el algoritmo de reducción para usar instancias de perdidad individual. Por defecto esto es auto, que es equivalente a \"SUM_OVER_BATCH_SIZE\". La perdida sera la suma de las perdidas de la instancia, ponderada por los pesos de las muestras, si este valor existe. Luego se lo divide por el tamaño del batch (no por la suma de pesos, no es un promedio ponderado). Otro posible valor es \"SUM\" o \"NONE\"\n",
    "\n",
    "El método call(), toma las etiquetas y las predicciones, computa las pérdidas de la instancia y las retorna.\n",
    "\n",
    "El método get_config() retorna un diccionario con cada hyperparámetro y su valor, primero llama al get_config() de la clase padre\n",
    "\n",
    "Ahora cuando se guarda el modelo, keras llama al método get_config() de la instancia de la clase y guarda la configuración en el SavedModel. Cuando se carga el modelo, llama al método from_config(), que de la clase HuberLoss, crea una instancia de la clase y le pasa **config al constructor. Y con esto se implementan las funciones de pérdida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b3f054-a159-4a06-be49-5cfbed33332f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
