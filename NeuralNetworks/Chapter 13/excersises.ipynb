{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18de52cd-fea9-4bb6-b36e-c53b3835f39b",
   "metadata": {},
   "source": [
    "## Por qu√© usar la api de tf.data?\n",
    "\n",
    "Permite aprovechar paralelizaci√≥n en el momento de cargar los datos, aprovechando la GPU y cargando los datos a la vez que se entrena el modelo, para poder evitar perder tiempo. Este paralelismo permite leer datos de m√∫ltiples archivos a la vez utilizando tecnicas de batching, shuffling. Mientras la GPU est√° entrenando el modelo aprovecha la CPU para cargar y pre procesar el pr√≥ximo lote de datos.\n",
    "\n",
    "Tambi√©n permite procesar datasets que no entran en la RAM, y aprovecha al m√°ximo todos los recursos. Puede leer csvs, binary files con largos establecidos de registros y binary files que usan Tensor Flow TFR Record format que soporta registros de varios tama√±os.\n",
    "\n",
    "TF.Data api tambi√©n soporta leer datos desde SQL. Tambi√©n brinda las capas de pre procesamiento de datos que se pueden incluir en el modelo para evitar diferencias entre desarrollo y producci√≥n.\n",
    "\n",
    "Ingesting a large dataset and preprocessing it efficiently can be a complex engineering challenge. The Data API makes it fairly simple. It offers many features, including loading data from various sources (such as text or binary files), reading data in parallel from multiple sources, transforming it, interleaving the records, shuffling the data, batching it, and prefetching it.\n",
    "\n",
    "## Cuales son los beneficios de dividir un dataset pesado en m√∫ltiples archivos?\n",
    "\n",
    "El descenso de gradiente funciona mejor cuando las instancias son independientes e identicamente distribuidas. Una forma de hacerlo es con el m√©todo shuffle, que va llenando un buffer (que es un dataset nuevo) con instancias del dataset inicial, cada vez que se necesite una instancia se saca una del dataset inicial hasta que se haya recorrido todo el dataset. El buffer debe tener el tama√±o adecuado sino es lo mismo que no hacerlo.\n",
    "\n",
    "El beneficio de tener m√∫ltiples archivos es evitar el orden intrinseco de los datos en el archivo inicial, para ello se divide en varios archivos, se hace shuffle, y luego se intercala el orden de archivos para obtener instancias. Esto permite manejar datasets gigantes que no entran en una m√°quina sola, es m√°s f√°cil dividir los datos en m√∫ltiples subsets que manipular un dataset gigante.\n",
    "\n",
    "Splitting a large dataset into multiple files makes it possible to shuffle it at a coarse level before shuffling it at a finer level using a shuffling buffer. It also makes it possible to handle huge datasets that do not fit on a single machine. It's also simpler to manipulate thousands of small files rather than one huge file; for example, it's easier to split the data into multiple subsets. Lastly, if the data is split across multiple files spread across multiple servers, it is possible to download several files from different servers simultaneously, which improves the bandwidth usage.\n",
    "\n",
    "## Durante el entrenamiento, como se puede identificar que el input pipeline est√° causando demoras?\n",
    "Se puede usar TensorBoard para checkear si la GPU est√° siendo utilizada al m√°ximo, de lo contrario existe holgura entre el tiempo de procesamiento de los datos que impide el entrenamiento contin√∫o, hay que asegurarse de que se lea y preprocese los datos en paralelo y m√∫ltiples hilos. Si esto es insuficiente, puede ser que el pre procesamiento no sea eficiente, sino se puede probar guardandooslo el dataset en m√∫ltiples TFRecord files, y pre procesando con antelaci√≥n as√≠ no se tiene que hacer durante el entrenamiento. Si todo esto no es necesario hay que aumentar la capacidad de computo. \n",
    "\n",
    "You can use TensorBoard to visualize profiling data: if the GPU is not fully utilized then your input pipeline is likely to be the bottleneck. You can fix it by making sure it reads and preprocesses the data in multiple threads in parallel, and ensuring it prefetches a few batches. If this is insufficient to get your GPU to 100% usage during training, make sure your preprocessing code is optimized. You can also try saving the dataset into multiple TFRecord files, and if necessary perform some of the preprocessing ahead of time so that it does not need to be done on the fly during training (TF Transform can help with this). If necessary, use a machine with more CPU and RAM, and ensure that the GPU bandwidth is large enough.\n",
    "\n",
    "## Se puede guardar cualquier tipo de dato en un archivo TFRecord o solo buffers de protocolo serializados?\n",
    "Si, cualquier tipo binario de dato se puede guardar en un TFRecord, son una secuencia de archivos binarios arbitrarios. En la practica la mayoria contiene potobufs porque esto aprovecha las ventajas de acceso multiplataforma, lenguaje y actualizaci√≥n de definici√≥n. \n",
    "\n",
    "A TFRecord file is composed of a sequence of arbitrary binary records: you can store absolutely any binary data you want in each record. However, in practice most TFRecord files contain sequences of serialized protocol buffers. This makes it possible to benefit from the advantages of protocol buffers, such as the fact that they can be read easily across multiple platforms and languages and their definition can be updated later in a backward-compatible way.\n",
    "\n",
    "## Why would you go through the hassle of converting all your data to the Example protobuf format? Why not use your own protobuf definition?\n",
    "El formato ejemplo de protobuf tiene mucha compatibil√≠dad por parte de TensorFlow que ya incluye muchas operaciones para usarlo en `tf.io.parse`*`example()`. El formato es lo suficientemente flexible para representar la mayor√≠a de los datasets, y adem√°s permite tener compatibil√≠dad facilmente con otros lenguajes.\n",
    "\n",
    "En caso de usar mi propia definici√≥n de protobuf, hay que compilar usando protoc y `--descriptor_set_out` and `--include_imports` para exportar la descripci√≥n del tipo y usar `tf.io.decode_proto()` para parsear la definici√≥n del protobuf, esto requiere luego deployear la definici√≥n con el modelo.\n",
    "\n",
    "The `Example` protobuf format has the advantage that TensorFlow provides some operations to parse it (the `tf.io.parse`*`example()` functions) without you having to define your own format. It is sufficiently flexible to represent instances in most datasets. However, if it does not cover your use case, you can define your own protocol buffer, compile it using `protoc` (setting the `--descriptor_set_out` and `--include_imports` arguments to export the protobuf descriptor), and use the `tf.io.decode_proto()` function to parse the serialized protobufs (see the \"Custom protobuf\" section of the notebook for an example). It's more complicated, and it requires deploying the descriptor along with the model, but it can be done.\n",
    "\n",
    "## When using TFRecords, when would you want to activate compression? Why not do it systematically?\n",
    "Se activa compresi√≥n cuando el script de entrenamiento requiere descargar el dataset, entonces es viable reducir el tiempo de descarga. Si solo se va a utilizar localmente no es viable porque a√±ade tiempo de descompresi√≥n. \n",
    "\n",
    "When using TFRecords, you will generally want to activate compression if the TFRecord files will need to be downloaded by the training script, as compression will make files smaller and thus reduce download time. But if the files are located on the same machine as the training script, it's usually preferable to leave compression off, to avoid wasting CPU for decompression.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520c3e30-f9e4-448a-add3-6a88d4b3b742",
   "metadata": {},
   "source": [
    "## Data can be preprocessed directly when writing the data files, or within the tf.data pipeline, or in preprocessing layers within your model. Can you list a few pros and cons of each option?\n",
    "\n",
    "# üß† Where to Preprocess Your Data in TensorFlow\n",
    "\n",
    "Data can be preprocessed at **three different stages** in a TensorFlow workflow:  \n",
    "1. When writing files (offline, before training)  \n",
    "2. Inside the `tf.data` input pipeline (on the fly)  \n",
    "3. Inside preprocessing layers in the model  \n",
    "\n",
    "Each approach has clear pros and cons.\n",
    "\n",
    "---\n",
    "\n",
    "## üèóÔ∏è 1. Preprocessing When Writing Data Files (Offline)\n",
    "\n",
    "### ‚úÖ Pros\n",
    "- **Fastest training input:** data is already transformed ‚Äî minimal pipeline overhead.  \n",
    "- **Deterministic & repeatable:** once written, always the same.  \n",
    "- **Framework-agnostic:** the data can be consumed by TensorFlow, PyTorch, or others.  \n",
    "- **Efficient for heavy transforms:** feature engineering, joins, or global aggregations.\n",
    "\n",
    "### ‚ùå Cons\n",
    "- **Inflexible:** any change in preprocessing requires rewriting the dataset.  \n",
    "- **Storage cost:** may need to keep both raw and processed copies.  \n",
    "- **Potential data leakage:** if statistics were computed using train+test data.  \n",
    "- **Versioning overhead:** must track preprocessing + dataset versions manually.\n",
    "\n",
    "### üí° Best for\n",
    "- Huge, stable datasets.  \n",
    "- Heavy preprocessing that rarely changes.  \n",
    "- Production pipelines (TFX, Beam, Spark) that materialize features once.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è 2. Preprocessing in `tf.data` Pipeline (On-the-Fly)\n",
    "\n",
    "### ‚úÖ Pros\n",
    "- **Flexible:** change transforms without rewriting data.  \n",
    "- **Efficient:** runs in parallel, can be cached and prefetched.  \n",
    "- **Keeps raw source intact:** easier to reproduce experiments.  \n",
    "- **Perfect for augmentation:** random transformations per epoch (flips, crops, etc.).\n",
    "\n",
    "### ‚ùå Cons\n",
    "- **Training-only by default:** must reimplement preprocessing for inference.  \n",
    "- **Possible bottleneck:** if heavy transforms aren‚Äôt parallelized.  \n",
    "- **Harder debugging:** graph mode errors, silent performance issues.\n",
    "\n",
    "### üí° Best for\n",
    "- Rapid experimentation.  \n",
    "- Data augmentation that changes every epoch.  \n",
    "- When you control separate inference preprocessing elsewhere.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© 3. Preprocessing in Model Layers (Within the Graph)\n",
    "\n",
    "### ‚úÖ Pros\n",
    "- **Training‚Äìserving parity:** preprocessing saved *inside* the model.  \n",
    "- **Portable:** works on TensorFlow Serving, TF Lite, and other deployments.  \n",
    "- **Runs on accelerators:** can execute on GPU/TPU if lightweight.  \n",
    "- **Self-contained:** statistics (mean, std) stored in model weights.\n",
    "\n",
    "### ‚ùå Cons\n",
    "- **Heavier preprocessing slows training:** better to move expensive steps outside.  \n",
    "- **Limited expressiveness:** not ideal for complex joins or dataset-level operations.  \n",
    "- **Requires model re-export:** every time preprocessing logic changes.\n",
    "\n",
    "### üí° Best for\n",
    "- Lightweight per-example transforms (normalization, tokenization).  \n",
    "- Serving in multiple environments without separate pipelines.  \n",
    "- Ensuring consistent preprocessing between training and inference.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öñÔ∏è Quick Decision Guide\n",
    "\n",
    "| Type of Transform | Recommended Location |\n",
    "|--------------------|----------------------|\n",
    "| Heavy global feature engineering | Write-time |\n",
    "| Random augmentation (flip, crop, noise) | `tf.data` pipeline |\n",
    "| Per-example normalization / tokenization | Model preprocessing layer |\n",
    "| You need one deployable model artifact | Model preprocessing layer |\n",
    "| Rapid iteration during training | `tf.data` pipeline |\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Real-World Hybrid Strategy\n",
    "\n",
    "Most production pipelines mix all three:\n",
    "\n",
    "1. **Offline:** heavy, global transforms (joins, aggregations, stable feature engineering).  \n",
    "2. **`tf.data`:** lightweight, stochastic, or per-epoch augmentations.  \n",
    "3. **Model layers:** final normalization/tokenization to guarantee parity at serving time.\n",
    "\n",
    "This approach balances **speed, flexibility, and reliability** across both training and deployment.\n",
    "\n",
    "\n",
    "# üß† D√≥nde Preprocesar Tus Datos en TensorFlow\n",
    "\n",
    "Los datos pueden preprocesarse en **tres etapas diferentes** dentro de un flujo de trabajo con TensorFlow:  \n",
    "1. Al escribir los archivos (offline, antes del entrenamiento)  \n",
    "2. Dentro del pipeline de entrada `tf.data` (en tiempo real)  \n",
    "3. Dentro de las capas de preprocesamiento del modelo  \n",
    "\n",
    "Cada enfoque tiene ventajas y desventajas claras.\n",
    "\n",
    "---\n",
    "\n",
    "## üèóÔ∏è 1. Preprocesamiento al Escribir los Archivos (Offline)\n",
    "\n",
    "### ‚úÖ Ventajas\n",
    "- **Entrenamiento m√°s r√°pido:** los datos ya est√°n transformados, m√≠nimo costo en el pipeline.  \n",
    "- **Determinista y reproducible:** una vez escrito, siempre se comporta igual.  \n",
    "- **Agn√≥stico al framework:** los datos pueden ser consumidos por TensorFlow, PyTorch, etc.  \n",
    "- **Eficiente para transformaciones pesadas:** ingenier√≠a de features, joins o agregaciones globales.\n",
    "\n",
    "### ‚ùå Desventajas\n",
    "- **Poco flexible:** cualquier cambio requiere reescribir el dataset.  \n",
    "- **Mayor consumo de almacenamiento:** puede ser necesario guardar la versi√≥n cruda y la procesada.  \n",
    "- **Riesgo de fuga de informaci√≥n:** si las estad√≠sticas se calcularon con datos de entrenamiento + prueba.  \n",
    "- **Gesti√≥n de versiones:** hay que versionar dataset y transformaciones manualmente.\n",
    "\n",
    "### üí° Ideal cuando\n",
    "- Los datasets son grandes y estables.  \n",
    "- Las transformaciones son costosas pero poco cambiantes.  \n",
    "- Ten√©s un pipeline de producci√≥n (TFX, Beam, Spark) que materializa las features una sola vez.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è 2. Preprocesamiento en el Pipeline `tf.data` (En Tiempo Real)\n",
    "\n",
    "### ‚úÖ Ventajas\n",
    "- **Flexible:** pod√©s cambiar transformaciones sin reescribir los datos.  \n",
    "- **Eficiente:** soporta paralelizaci√≥n, cache y prefetch.  \n",
    "- **Conserva la fuente cruda:** facilita reproducir experimentos.  \n",
    "- **Ideal para aumentaci√≥n:** transformaciones aleatorias por √©poca (rotar, cortar, etc.).\n",
    "\n",
    "### ‚ùå Desventajas\n",
    "- **Solo para entrenamiento:** se debe reimplementar el preprocesamiento para inferencia.  \n",
    "- **Puede ser un cuello de botella:** si no se paraleliza correctamente.  \n",
    "- **M√°s dif√≠cil de depurar:** errores en modo gr√°fico o bajo rendimiento silencioso.\n",
    "\n",
    "### üí° Ideal cuando\n",
    "- Est√°s iterando r√°pido en los features.  \n",
    "- Us√°s aumentaci√≥n aleatoria de datos.  \n",
    "- Pod√©s mantener una l√≥gica de preprocesamiento separada para inferencia.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© 3. Preprocesamiento en Capas del Modelo (Dentro del Grafo)\n",
    "\n",
    "### ‚úÖ Ventajas\n",
    "- **Paridad entrenamiento‚Äìserving:** el preprocesamiento se guarda dentro del modelo.  \n",
    "- **Portabilidad:** funciona en TensorFlow Serving, TF Lite, etc.  \n",
    "- **Acelerado:** puede ejecutarse en GPU/TPU si las operaciones son livianas.  \n",
    "- **Autocontenido:** las estad√≠sticas (media, desv√≠o) se guardan como pesos del modelo.\n",
    "\n",
    "### ‚ùå Desventajas\n",
    "- **Puede ralentizar el entrenamiento:** si se incluyen transformaciones pesadas.  \n",
    "- **Limitaciones expresivas:** no es ideal para joins complejos o operaciones a nivel dataset.  \n",
    "- **Requiere reexportar el modelo:** cada vez que cambia la l√≥gica de preprocesamiento.\n",
    "\n",
    "### üí° Ideal cuando\n",
    "- Las transformaciones son ligeras y por ejemplo (normalizaci√≥n, tokenizaci√≥n).  \n",
    "- Necesit√°s un modelo autocontenido que funcione igual en producci√≥n.  \n",
    "- Quer√©s evitar mantener pipelines de preprocesamiento externos.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öñÔ∏è Gu√≠a R√°pida de Decisi√≥n\n",
    "\n",
    "| Tipo de Transformaci√≥n | Lugar Recomendado |\n",
    "|-------------------------|------------------|\n",
    "| Ingenier√≠a de features pesada y global | Escribir (offline) |\n",
    "| Aumentaci√≥n aleatoria (flip, crop, ruido) | Pipeline `tf.data` |\n",
    "| Normalizaci√≥n / tokenizaci√≥n por ejemplo | Capa de preprocesamiento del modelo |\n",
    "| Necesit√°s un √∫nico artefacto desplegable | Capa de preprocesamiento del modelo |\n",
    "| Iteraciones r√°pidas en entrenamiento | Pipeline `tf.data` |\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Estrategia H√≠brida (la m√°s usada en producci√≥n)\n",
    "\n",
    "La mayor√≠a de los pipelines combinan los tres niveles:\n",
    "\n",
    "1. **Offline:** transformaciones globales o costosas (joins, agregaciones, features estables).  \n",
    "2. **`tf.data`:** transformaciones ligeras o aleatorias por √©poca.  \n",
    "3. **Capas del modelo:** normalizaci√≥n o tokenizaci√≥n final para asegurar paridad entre entrenamiento y serving.\n",
    "\n",
    "Este enfoque logra el equilibrio ideal entre **velocidad, flexibilidad y confiabilidad** tanto en entrenamiento como en despliegue.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb953ce-438a-4172-9a80-5757b1577f1e",
   "metadata": {},
   "source": [
    "## Name a few common ways you can encode categorical integer features. What about text?\n",
    "\n",
    "Let's look at how to encode categorical text features and text:\n",
    "    * To encode a categorical feature that has a natural order, such as a movie rating (e.g., \"bad,\" \"average,\" \"good\"), the simplest option is to use ordinal encoding: sort the categories in their natural order and map each category to its rank (e.g., \"bad\" maps to 0, \"average\" maps to 1, and \"good\" maps to 2). However, most categorical features don't have such a natural order. For example, there's no natural order for professions or countries. In this case, you can use one-hot encoding, or embeddings if there are many categories. With Keras, the `StringLookup` layer can be used for ordinal encoding (using the default `output_mode=\"int\"`), or one-hot encoding (using `output_mode=\"one_hot\"`). It can also perform multi-hot encoding (using `output_mode=\"multi_hot\"`) if you want to encode multiple categorical text features together, assuming they share the same categories and it doesn't matter which feature contributed which category. For trainable embeddings, you must first use the `StringLookup` layer to produce an ordinal encoding, then use the `Embedding` layer.\n",
    "    * For text, the `TextVectorization` layer is easy to use and it can work well for simple tasks, or you can use TF Text for more advanced features. However, you'll often want to use pretrained language models, which you can obtain using tools like TF Hub or Hugging Face's Transformers library. These last two options are discussed in Chapter 16.\n",
    "\n",
    "# üß© Gu√≠a de Codificaci√≥n de Datos Categ√≥ricos y Texto en TensorFlow\n",
    "\n",
    "Cuando trabaj√°s con Machine Learning, los modelos solo entienden **n√∫meros**, as√≠ que necesitamos convertir las categor√≠as o el texto en representaciones num√©ricas.  \n",
    "A continuaci√≥n se detallan los m√©todos m√°s comunes para **features categ√≥ricas** y **texto**, junto con sus ventajas y cu√°ndo usarlos.\n",
    "\n",
    "---\n",
    "\n",
    "## üî¢ Codificaci√≥n de Features Categ√≥ricas (Enteros)\n",
    "\n",
    "Supongamos que ten√©s una columna con categor√≠as representadas por n√∫meros enteros como `[0, 1, 2, 3]` (por ejemplo, tipo de producto o pa√≠s).\n",
    "\n",
    "---\n",
    "\n",
    "### 1Ô∏è‚É£ One-Hot Encoding\n",
    "\n",
    "Cada categor√≠a se representa como un vector binario donde solo una posici√≥n es 1.\n",
    "\n",
    "| Valor | Vector codificado |\n",
    "|--------|-------------------|\n",
    "| 0 | [1, 0, 0, 0] |\n",
    "| 1 | [0, 1, 0, 0] |\n",
    "| 2 | [0, 0, 1, 0] |\n",
    "| 3 | [0, 0, 0, 1] |\n",
    "\n",
    "**Ventajas**\n",
    "- ‚úÖ Simple y efectivo para pocas categor√≠as.  \n",
    "- ‚úÖ F√°cil de interpretar.\n",
    "\n",
    "**Desventajas**\n",
    "- ‚ùå Poco eficiente si hay muchas categor√≠as (vectores muy grandes).  \n",
    "\n",
    "**En TensorFlow**\n",
    "```python\n",
    "tf.keras.layers.CategoryEncoding(num_tokens=4, output_mode=\"one_hot\")\n",
    "````\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ Embeddings\n",
    "\n",
    "Cada categor√≠a entera se representa mediante un vector denso de dimensi√≥n fija, aprendido durante el entrenamiento.\n",
    "\n",
    "| Categor√≠a | Embedding (ejemplo) |\n",
    "| --------- | ------------------- |\n",
    "| 0         | [0.2, -0.5, 0.8]    |\n",
    "| 1         | [0.7, 0.1, -0.3]    |\n",
    "\n",
    "**Ventajas**\n",
    "\n",
    "* ‚úÖ Escalable para miles de categor√≠as.\n",
    "* ‚úÖ Captura similitud entre categor√≠as.\n",
    "\n",
    "**Desventajas**\n",
    "\n",
    "* ‚ùå Requiere entrenamiento para aprender las representaciones.\n",
    "\n",
    "**En TensorFlow**\n",
    "\n",
    "```python\n",
    "tf.keras.layers.Embedding(input_dim=num_categorias, output_dim=embedding_dim)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ Codificaci√≥n Ordinal (Directa)\n",
    "\n",
    "Se utilizan los enteros tal cual (`0, 1, 2, ...`) como valores num√©ricos.\n",
    "\n",
    "**Ventajas**\n",
    "\n",
    "* ‚úÖ Muy simple.\n",
    "* ‚úÖ Sin memoria adicional.\n",
    "\n",
    "**Desventajas**\n",
    "\n",
    "* ‚ùå Solo tiene sentido si el orden es sem√°ntico.\n",
    "* ‚ùå El modelo puede interpretar distancias inexistentes entre categor√≠as.\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Codificaci√≥n de Texto\n",
    "\n",
    "El texto se compone de secuencias de palabras o caracteres.\n",
    "Por lo tanto, requiere pasos adicionales de tokenizaci√≥n y representaci√≥n.\n",
    "\n",
    "---\n",
    "\n",
    "### 1Ô∏è‚É£ Tokenizaci√≥n\n",
    "\n",
    "Convierte las palabras o subpalabras en √≠ndices enteros.\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "```\n",
    "\"El perro duerme\" ‚Üí [1, 42, 305]\n",
    "```\n",
    "\n",
    "**En TensorFlow**\n",
    "\n",
    "```python\n",
    "tf.keras.layers.TextVectorization(max_tokens=10000)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ One-Hot o Multi-Hot (para texto corto)\n",
    "\n",
    "Se representa la **presencia o ausencia** de palabras en un vocabulario como vectores binarios.\n",
    "\n",
    "| Palabra | Vector            |\n",
    "| ------- | ----------------- |\n",
    "| ‚Äúperro‚Äù | [0, 1, 0, 0, ...] |\n",
    "| ‚Äúgato‚Äù  | [0, 0, 1, 0, ...] |\n",
    "\n",
    "**Ventajas**\n",
    "\n",
    "* ‚úÖ Simple e interpretable.\n",
    "\n",
    "**Desventajas**\n",
    "\n",
    "* ‚ùå No conserva el orden de las palabras ni el contexto.\n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ Embeddings de Palabras\n",
    "\n",
    "Cada palabra o token se representa con un vector denso (preentrenado o aprendido).\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "```\n",
    "\"perro\" ‚Üí [0.11, -0.34, 0.87, ...]\n",
    "```\n",
    "\n",
    "**Ventajas**\n",
    "\n",
    "* ‚úÖ Captura similitud sem√°ntica (‚Äúperro‚Äù ‚âà ‚Äúgato‚Äù).\n",
    "* ‚úÖ Escalable.\n",
    "\n",
    "**Desventajas**\n",
    "\n",
    "* ‚ùå Requiere entrenamiento o embeddings preentrenados (Word2Vec, GloVe, etc.).\n",
    "\n",
    "**En TensorFlow**\n",
    "\n",
    "```python\n",
    "tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4Ô∏è‚É£ Representaciones Contextuales (Modelos Modernos)\n",
    "\n",
    "Modelos como **BERT**, **GPT**, **WordPiece** o **SentencePiece** generan embeddings que dependen del contexto de toda la oraci√≥n.\n",
    "\n",
    "**Ventajas**\n",
    "\n",
    "* ‚úÖ Capturan significado contextual preciso.\n",
    "* ‚úÖ Excelente rendimiento en tareas de NLP.\n",
    "\n",
    "**Desventajas**\n",
    "\n",
    "* ‚ùå Elevado costo computacional.\n",
    "* ‚ùå Requieren mucha memoria y procesamiento.\n",
    "\n",
    "---\n",
    "\n",
    "## üß≠ Resumen R√°pido\n",
    "\n",
    "| Tipo de dato            | M√©todos comunes                                                | Cu√°ndo usarlos                                                                          |\n",
    "| ----------------------- | -------------------------------------------------------------- | --------------------------------------------------------------------------------------- |\n",
    "| **Categ√≥ricos enteros** | One-Hot, Embedding, Ordinal                                    | Pocas categor√≠as ‚Üí One-Hot. Muchas categor√≠as ‚Üí Embedding.                              |\n",
    "| **Texto**               | Tokenizaci√≥n + Embedding / Bag-of-Words / Modelos contextuales | Texto corto o simple ‚Üí Tokenizaci√≥n + Embedding. Texto complejo ‚Üí Modelos contextuales. |\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Ejemplo de Flujo Combinado\n",
    "\n",
    "Un modelo puede incluir ambos tipos de codificaci√≥n:\n",
    "\n",
    "```python\n",
    "# Feature categ√≥rica\n",
    "categoria = tf.keras.Input(shape=(1,), dtype=tf.int32)\n",
    "categoria_encoded = tf.keras.layers.Embedding(input_dim=50, output_dim=8)(categoria)\n",
    "\n",
    "# Texto\n",
    "texto = tf.keras.Input(shape=(1,), dtype=tf.string)\n",
    "vectorizer = tf.keras.layers.TextVectorization(max_tokens=10000, output_sequence_length=100)\n",
    "texto_vectorizado = vectorizer(texto)\n",
    "texto_embedded = tf.keras.layers.Embedding(10000, 16)(texto_vectorizado)\n",
    "\n",
    "# Concatenar y continuar el modelo\n",
    "concat = tf.keras.layers.Concatenate()([tf.keras.layers.Flatten()(categoria_encoded),\n",
    "                                        tf.keras.layers.Flatten()(texto_embedded)])\n",
    "salida = tf.keras.layers.Dense(1, activation=\"sigmoid\")(concat)\n",
    "\n",
    "modelo = tf.keras.Model(inputs=[categoria, texto], outputs=salida)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "üëâ **Conclusi√≥n:**\n",
    "Us√° **One-Hot** o **Embeddings** para categor√≠as, y **Tokenizaci√≥n + Embeddings** para texto.\n",
    "Si necesit√°s entender contexto o sem√°ntica avanzada, **modelos preentrenados** como BERT o GPT son la mejor opci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af228bd2-08af-457a-8903-db6b4db91ff7",
   "metadata": {},
   "source": [
    "To encode a categorical feature that has a natural order, such as a movie rating (e.g., \"bad,\" \"average,\" \"good\"), the simplest option is to use ordinal encoding: sort the categories in their natural order and map each category to its rank (e.g., \"bad\" maps to 0, \"average\" maps to 1, and \"good\" maps to 2). However, most categorical features don't have such a natural order. For example, there's no natural order for professions or countries. In this case, you can use one-hot encoding, or embeddings if there are many categories. With Keras, the StringLookup layer can be used for ordinal encoding (using the default output_mode=\"int\"), or one-hot encoding (using output_mode=\"one_hot\"). It can also perform multi-hot encoding (using output_mode=\"multi_hot\") if you want to encode multiple categorical text features together, assuming they share the same categories and it doesn't matter which feature contributed which category. For trainable embeddings, you must first use the StringLookup layer to produce an ordinal encoding, then use the Embedding layer.\n",
    "For text, the TextVectorization layer is easy to use and it can work well for simple tasks, or you can use TF Text for more advanced features. However, you'll often want to use pretrained language models, which you can obtain using tools like TF Hub or Hugging Face's Transformers library. These last two options are discussed in Chapter 16.\n",
    "\n",
    "Load the Fashion MNIST dataset (introduced in Chapter 10); split it into a training set, a validation set, and a test set; shuffle the training set; and save each dataset to multiple TFRecord files. Each record should be a serialized Example protobuf with two features: the serialized image (use tf.io.serialize_tensor() to serialize each image), and the label.‚Å† Then use tf.data to create an efficient dataset for each set. Finally, use a Keras model to train these datasets, including a preprocessing layer to standardize each input feature. Try to make the input pipeline as efficient as possible, using TensorBoard to visualize profiling data.\n",
    "\n",
    "‚óè apply()              Applies a transformation function to this dataset.\n",
    "‚óè as_numpy_iterator()  Returns an iterator which converts all elements of the dataset to numpy.\n",
    "‚óè batch()              Combines consecutive elements of this dataset into batches.\n",
    "‚óè bucket_by_sequence_length()A transformation that buckets elements in a `Dataset` by length.\n",
    "‚óè cache()              Caches the elements in this dataset.\n",
    "‚óè cardinality()        Returns the cardinality of the dataset, if known.\n",
    "‚óè choose_from_datasets()Creates a dataset that deterministically chooses elements from `datasets`.\n",
    "‚óè concatenate()        Creates a `Dataset` by concatenating the given dataset with this dataset.\n",
    "‚óè element_spec()       The type specification of an element of this dataset.\n",
    "‚óè enumerate()          Enumerates the elements of this dataset.\n",
    "‚óè filter()             Filters this dataset according to `predicate`.\n",
    "‚óè flat_map()           Maps `map_func` across this dataset and flattens the result.\n",
    "‚óè from_generator()     Creates a `Dataset` whose elements are generated by `generator`. (deprecated arguments)\n",
    "‚óè from_tensor_slices() Creates a `Dataset` whose elements are slices of the given tensors.\n",
    "‚óè from_tensors()       Creates a `Dataset` with a single element, comprising the given tensors.\n",
    "‚óè get_single_element() Returns the single element of the `dataset`.\n",
    "‚óè group_by_window()    Groups windows of elements by key and reduces them.\n",
    "‚óè interleave()         Maps `map_func` across this dataset, and interleaves the results.\n",
    "‚óè list_files()         A dataset of all files matching one or more glob patterns.\n",
    "‚óè map()                Maps `map_func` across the elements of this dataset.\n",
    "‚óè options()            Returns the options for this dataset and its inputs.\n",
    "‚óè padded_batch()       Combines consecutive elements of this dataset into padded batches.\n",
    "‚óè prefetch()           Creates a `Dataset` that prefetches elements from this dataset.\n",
    "‚óè random()             Creates a `Dataset` of pseudorandom values.\n",
    "‚óè range()              Creates a `Dataset` of a step-separated range of values.\n",
    "‚óè reduce()             Reduces the input dataset to a single element.\n",
    "‚óè rejection_resample() A transformation that resamples a dataset to a target distribution.\n",
    "‚óè repeat()             Repeats this dataset so each original value is seen `count` times.\n",
    "‚óè sample_from_datasets()Samples elements at random from the datasets in `datasets`.\n",
    "‚óè scan()               A transformation that scans a function across an input dataset.\n",
    "‚óè shard()              Creates a `Dataset` that includes only 1/`num_shards` of this dataset.\n",
    "‚óè shuffle()            Randomly shuffles the elements of this dataset.\n",
    "‚óè skip()               Creates a `Dataset` that skips `count` elements from this dataset.\n",
    "‚óè snapshot()           API to persist the output of the input dataset.\n",
    "‚óè take()               Creates a `Dataset` with at most `count` elements from this dataset.\n",
    "‚óè take_while()         A transformation that stops dataset iteration based on a `predicate`.\n",
    "‚óè unbatch()            Splits elements of a dataset into multiple elements.\n",
    "‚óè unique()             A transformation that discards duplicate elements of a `Dataset`.\n",
    "‚óè window()             Returns a dataset of \"windows\".\n",
    "‚óè with_options()       Returns a new `tf.data.Dataset` with the given options set.\n",
    "‚óè zip()                Creates a `Dataset` by zipping together the given datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d35c410-ba90-4a29-957a-33bb6aa65c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-13 19:10:45.538750: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "2025-10-13 19:10:52.837290: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "2025-10-13 19:10:53.528609: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.train import BytesList, FloatList, Int64List\n",
    "from tensorflow.train import Feature, Features, Example\n",
    "from contextlib import ExitStack\n",
    "\n",
    "\n",
    "(X_train_full, y_train_full), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "train_set = tf.data.Dataset.from_tensor_slices((X_train, y_train)) #No supe concatenaros los X_train e y_train\n",
    "train_set = train_set.shuffle(len(X_train), seed=42)\n",
    "valid_set = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))\n",
    "test_set = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "\n",
    "files_quantity = 5\n",
    "\n",
    "def convert_image(image, label):\n",
    "    image_bytes = tf.io.serialize_tensor(image).numpy()\n",
    "    \n",
    "    image_example = Example(\n",
    "        features=Features(\n",
    "            feature={\n",
    "                \"image\": Feature(bytes_list=BytesList(value=[image_bytes])),\n",
    "                \"label\": Feature(int64_list=Int64List(value=[label]))\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return image_example\n",
    "\n",
    "def write_tfrecords(name, dataset, n_shards=10):\n",
    "    paths = [\"{}.tfrecord-{:05d}-of-{:05d}\".format(name, index, n_shards)\n",
    "             for index in range(n_shards)]\n",
    "    with ExitStack() as stack:\n",
    "        writers = [stack.enter_context(tf.io.TFRecordWriter(path))\n",
    "                   for path in paths]\n",
    "        for index, (image, label) in dataset.enumerate():\n",
    "            shard = index % n_shards\n",
    "            example = convert_image(image, label)\n",
    "            writers[shard].write(example.SerializeToString())\n",
    "    return paths\n",
    "\n",
    "train_filepaths = write_tfrecords(\"my_fashion_mnist.train\", train_set)\n",
    "valid_filepaths = write_tfrecords(\"my_fashion_mnist.valid\", valid_set)\n",
    "test_filepaths = write_tfrecords(\"my_fashion_mnist.test\", test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e871d91-c134-4dda-b8a0-1779fe6db8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_spec = {\n",
    "    \"image\": tf.io.FixedLenFeature([], tf.string),   # string escalar\n",
    "    \"label\": tf.io.FixedLenFeature([], tf.int64),\n",
    "}\n",
    "\n",
    "def parse(serialized):\n",
    "    ex = tf.io.parse_single_example(serialized, feature_spec)\n",
    "    image = tf.io.parse_tensor(ex[\"image\"], out_type=tf.uint8)  # deserializa al tensor original\n",
    "    image = tf.reshape(image, [28, 28])                         # ajusta a tu forma real\n",
    "    label = ex[\"label\"]\n",
    "    return image, label\n",
    "\n",
    "def mnist_dataset(filepaths, n_read_threads=5, shuffle_buffer_size=None,\n",
    "                  n_parse_threads=5, batch_size=32, cache=True):\n",
    "    dataset = tf.data.TFRecordDataset(filepaths,\n",
    "                                      num_parallel_reads=n_read_threads)\n",
    "    if cache:\n",
    "        dataset = dataset.cache()\n",
    "    if shuffle_buffer_size:\n",
    "        dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "    dataset = dataset.map(parse, num_parallel_calls=n_parse_threads)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset.prefetch(1)\n",
    "\n",
    "train_set = mnist_dataset(train_filepaths, shuffle_buffer_size=60000)\n",
    "valid_set = mnist_dataset(valid_filepaths)\n",
    "test_set = mnist_dataset(test_filepaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cdf211-1ec6-4131-8a80-9f4e5dd77c05",
   "metadata": {},
   "source": [
    "## Prefech()\n",
    "\n",
    "Porque prefetch(1) le dice a tf.data que prepare por adelantado exactamente un lote mientras tu modelo est√° entrenando con el lote anterior. Eso crea superposici√≥n entre:\n",
    "la etapa de entrada (lectura, parseo, transformaciones en la unidad central de procesamiento), y\n",
    "la etapa de c√≥mputo del modelo (en la unidad de procesamiento gr√°fico o en la unidad central de procesamiento).\n",
    "Resultado: menos tiempo ocioso entre lotes y pasos de entrenamiento m√°s fluidos.\n",
    "Detalles r√°pidos:\n",
    "prefetch(1) mantiene un b√∫fer de un lote ‚Äúlisto para servir‚Äù.\n",
    "Si el lote actual tarda en computarse, el siguiente ya est√° parcialmente o totalmente preparado.\n",
    "Si pon√©s un n√∫mero muy alto, pod√©s ganar algo m√°s de rendimiento, pero consum√≠s m√°s memoria.\n",
    "Un valor de 1 es un default seguro y mantenible: casi siempre mejora el throughput sin riesgos de memoria.\n",
    "\n",
    "dataset = dataset.prefetch(tf.data.AUTOTUNE)  # deja que TensorFlow elija el b√∫fer √≥ptimo\n",
    "\n",
    "Dej√° tambi√©n num_parallel_reads=tf.data.AUTOTUNE y num_parallel_calls=tf.data.AUTOTUNE para que el sistema ajuste los paralelismos autom√°ticamente seg√∫n el hardware y la carga.\n",
    "\n",
    "## Cache() \n",
    "dataset.cache() guarda en una memoria intermedia los elementos que produce la tuber√≠a de tf.data a la altura exacta donde lo coloques. As√≠, la primera pasada ‚Äúllena‚Äù la memoria intermedia y las pasadas siguientes leen desde esa memoria intermedia en lugar de volver a leer y volver a procesar desde el origen.\n",
    "\n",
    "Con cache() sin argumentos: guarda los elementos en memoria del proceso (memoria de acceso aleatorio).\n",
    "Con cache(\"ruta/al/archivo\"): guarda los elementos en un archivo local en disco. Esto persiste entre ejecuciones si no borras el archivo.\n",
    "\n",
    "- Efecto en rendimiento\n",
    "La primera √©poca puede tardar lo mismo o un poco m√°s (est√° llenando la memoria intermedia). Desde la segunda √©poca en adelante, leer es mucho m√°s r√°pido porque evita relectura y reprocesado caro (por ejemplo, descompresi√≥n, parseo, decodificaci√≥n de im√°genes).\n",
    "- D√≥nde colocarlo importa\n",
    "Despu√©s de etapas costosas deterministas (por ejemplo, leer, descomprimir, parsear, decodificar) para no repetir ese trabajo.\n",
    "Antes de barajar si quer√©s un orden distinto cada √©poca. Si lo pones despu√©s de barajar, vas a repetir la misma secuencia en todas las √©pocas, porque ya qued√≥ cacheado en ese orden.\n",
    "Antes de aumentos aleatorios si quieres que los aumentos var√≠en cada √©poca. Si cacheas despu√©s de aplicar aumentos aleatorios, los aumentos quedan ‚Äúcongelados‚Äù desde la primera pasada.\n",
    "Memoria y tama√±o\n",
    "- Sin archivo: todo el conjunto que pasa por el punto de cacheo debe caber en memoria del proceso.\n",
    "- Con archivo: ocupa espacio en disco similar al tama√±o de los elementos ya transformados hasta ese punto (no suele estar comprimido).\n",
    "Para conjuntos muy grandes que no caben en memoria, usa la variante con archivo.\n",
    "Determinismo y aleatoriedad\n",
    "Cualquier operaci√≥n con aleatoriedad antes del cache() se fija desde la primera √©poca.\n",
    "La aleatoriedad despu√©s del cache() se vuelve a aplicar en cada √©poca.\n",
    "Recomendaci√≥n pr√°ctica y mantenible\n",
    "\n",
    "\n",
    "## Shuffle()\n",
    "shuffle(buffer_size) desordena los elementos de un tf.data.Dataset usando un b√∫fer de tama√±o fijo.\n",
    "C√≥mo funciona, paso a paso:\n",
    "- Se llena un b√∫fer con hasta buffer_size elementos le√≠dos desde la fuente.\n",
    "- Se emite aleatoriamente uno de los elementos del b√∫fer.\n",
    "- Se repone el hueco leyendo el siguiente elemento desde la fuente.\n",
    "- Se repite hasta agotar la fuente.\n",
    "\n",
    "Consecuencias pr√°cticas:\n",
    "- Aleatoriedad: Cuanto m√°s grande el buffer_size, mejor aproximaci√≥n a un ‚Äúshuffle global‚Äù.\n",
    "- Si buffer_size ‚âà tama√±o total del conjunto, el orden es casi completamente aleatorio.\n",
    "- Si es peque√±o, el desorden es ‚Äúlocal‚Äù (bueno para streams muy grandes o memoria limitada).\n",
    "- Memoria: El b√∫fer vive en memoria. Ajust√° buffer_size seg√∫n tu RAM.\n",
    "- √âpocas: Por defecto, el orden cambia en cada √©poca (reshuffle_each_iteration=True).\n",
    "- Si quer√©s que sea id√©ntico en todas las √©pocas, usa reshuffle_each_iteration=False y fija una semilla.\n",
    "- Semillas: ds = ds.shuffle(buffer_size=10000, seed=42, reshuffle_each_iteration=True)\n",
    "\n",
    "### Colocaci√≥n en la pipeline (importa mucho):\n",
    "Si pones cache() antes de shuffle(), vas a barajar en cada √©poca sin recalcular parseos/decodificaciones.\n",
    "Si pones cache() despu√©s de shuffle(), quedar√°s con el mismo orden en todas las √©pocas (porque ya cacheaste el orden barajado de la primera pasada).\n",
    "*Regla mantenible*: parse/decodificar ‚Üí cache ‚Üí shuffle ‚Üí map(aumentos aleatorios si quer√©s que cambien por √©poca) ‚Üí batch ‚Üí prefetch.\n",
    "\n",
    "Interacci√≥n con repeat():\n",
    "- Si hac√©s ds.repeat().shuffle(...), el barajado se aplica a un flujo infinito; us√° reshuffle_each_iteration=True.\n",
    "- Si hac√©s shuffle(...).repeat(), el primer barajado se ‚Äúpropaga‚Äù y puede que el orden no cambie si no rebaraj√°s por √©poca.\n",
    "\n",
    "En resumen: shuffle aleatoriza el orden usando un b√∫fer. M√°s b√∫fer = mejor aleatoriedad, m√°s memoria. Ub√≠calo antes de batch y despu√©s de cache para entrenamientos r√°pidos y reproducibles cuando lo necesites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aea9d569-a11b-480d-aa8f-01e032c1da44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for X, y in train_set.take(1):\n",
    "    for i in range(5):\n",
    "        plt.subplot(1, 5, i + 1)\n",
    "        plt.imshow(X[i].numpy(), cmap=\"binary\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(str(y[i].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "342ccded-9faf-4f1a-b7bd-af53e2aa0dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-13 19:10:55.837865: I tensorflow/tsl/profiler/lib/profiler_session.cc:104] Profiler session initializing.\n",
      "2025-10-13 19:10:55.837882: I tensorflow/tsl/profiler/lib/profiler_session.cc:119] Profiler session started.\n",
      "2025-10-13 19:10:55.837902: I tensorflow/tsl/profiler/lib/profiler_session.cc:131] Profiler session tear down.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    122/Unknown - 0s 2ms/step - loss: 0.7629 - accuracy: 0.7257"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-13 19:10:56.128960: I tensorflow/tsl/profiler/lib/profiler_session.cc:104] Profiler session initializing.\n",
      "2025-10-13 19:10:56.128974: I tensorflow/tsl/profiler/lib/profiler_session.cc:119] Profiler session started.\n",
      "2025-10-13 19:10:56.130936: I tensorflow/tsl/profiler/lib/profiler_session.cc:70] Profiler session collecting data.\n",
      "2025-10-13 19:10:56.133340: I tensorflow/tsl/profiler/lib/profiler_session.cc:131] Profiler session tear down.\n",
      "2025-10-13 19:10:56.134265: I tensorflow/tsl/profiler/rpc/client/save_profile.cc:144] Collecting XSpace to repository: my_logs/run_/20251013_191055/plugins/profile/2025_10_13_19_10_56/192.168.1.5.xplane.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4456 - accuracy: 0.8399 - val_loss: 0.3534 - val_accuracy: 0.8734\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3316 - accuracy: 0.8786 - val_loss: 0.3456 - val_accuracy: 0.8792\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2954 - accuracy: 0.8919 - val_loss: 0.3165 - val_accuracy: 0.8850\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2703 - accuracy: 0.9001 - val_loss: 0.3250 - val_accuracy: 0.8850\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2501 - accuracy: 0.9075 - val_loss: 0.3372 - val_accuracy: 0.8858\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x17ecea830>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "standardization = tf.keras.layers.Normalization(input_shape=[28, 28])\n",
    "\n",
    "sample_image_batches = train_set.take(100).map(lambda image, label: image)\n",
    "sample_images = np.concatenate(list(sample_image_batches.as_numpy_iterator()),\n",
    "                               axis=0).astype(np.float32)\n",
    "standardization.adapt(sample_images)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    standardization,\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "\n",
    "logs = Path() / \"my_logs\" / \"run_\" / datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=logs, histogram_freq=1, profile_batch=10)\n",
    "\n",
    "model.fit(train_set, epochs=5, validation_data=valid_set,\n",
    "          callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2ce3e4-b309-4a24-aca4-8989671fbe9b",
   "metadata": {},
   "source": [
    "Para abrir TensorBoard \n",
    "\n",
    "Desde CMD:\n",
    "     tensorboard --logdir my_logs --port 6006 --reload_interval 2\n",
    "\n",
    "Y entrar a http://localhost:6006\n",
    "\n",
    "\n",
    "O desde Jupyter:\n",
    "    \n",
    "    %load_ext tensorboard\n",
    "    \n",
    "    %tensorboard --logdir my_logs --port 6006 --reload_interval 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86ff2183-e61f-4232-aa99-1071bafb62f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorboard --logdir my_logs --port 6006 --reload_interval 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570861e5-1de8-4b43-91cc-10084b0891be",
   "metadata": {},
   "source": [
    "# Ejercicio 10\n",
    "\n",
    "Download the Large Movie Review Dataset, which contains 50,000 movie reviews from the Internet Movie Database (IMDb). The data is organized in two directories, train and test, each containing a pos subdirectory with 12,500 positive reviews and a neg subdirectory with 12,500 negative reviews. Each review is stored in a separate text file. There are other files and folders (including preprocessed bag-of-words versions), but we will ignore them in this exercise. Split the test set into a validation set (15,000) and a test set (10,000). \n",
    "\n",
    "Use tf.data to create an efficient dataset for each set. Create a binary classification model, using a TextVectorization layer to preprocess each review.\n",
    "\n",
    "Add an Embedding layer and compute the mean embedding for each review, multiplied by the square root of the number of words (see Chapter 16). \n",
    "\n",
    "This rescaled mean embedding can then be passed to the rest of your model. \n",
    "\n",
    "Train the model and see what accuracy you get. \n",
    "\n",
    "Try to optimize your pipelines to make training as fast as possible. Use TFDS to load the same dataset more easily: tfds.load(\"imdb_reviews\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "91e61dd8-f003-49c5-b95b-4775f9067653",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "root = \"https://ai.stanford.edu/~amaas/data/sentiment/\"\n",
    "filename = \"aclImdb_v1.tar.gz\"\n",
    "filepath = tf.keras.utils.get_file(filename, root + filename, extract=True,\n",
    "                                   cache_dir=\"./\")\n",
    "if \"_extracted\" in filepath:\n",
    "    path = Path(filepath) / \"aclImdb\"\n",
    "else:\n",
    "    path = Path(filepath).with_name(\"aclImdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1569d327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/aclImdb/\n",
      "    test/\n",
      "        neg/\n",
      "            0_2.txt\n",
      "            10000_4.txt\n",
      "            10001_1.txt\n",
      "            ...\n",
      "        pos/\n",
      "            0_10.txt\n",
      "            10000_7.txt\n",
      "            10001_9.txt\n",
      "            ...\n",
      "        labeledBow.feat\n",
      "        urls_neg.txt\n",
      "        urls_pos.txt\n",
      "    train/\n",
      "        neg/\n",
      "            0_3.txt\n",
      "            10000_4.txt\n",
      "            10001_4.txt\n",
      "            ...\n",
      "        pos/\n",
      "            0_9.txt\n",
      "            10000_8.txt\n",
      "            10001_10.txt\n",
      "            ...\n",
      "        unsup/\n",
      "            0_0.txt\n",
      "            10000_0.txt\n",
      "            10001_0.txt\n",
      "            ...\n",
      "        labeledBow.feat\n",
      "        unsupBow.feat\n",
      "        urls_neg.txt\n",
      "        ...\n",
      "    README\n",
      "    imdb.vocab\n",
      "    imdbEr.txt\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def tree(path, level=0, indent=4, max_files=3):\n",
    "    if level == 0:\n",
    "        print(f\"{path}/\")\n",
    "        level += 1\n",
    "    sub_paths = sorted(path.iterdir())\n",
    "    sub_dirs = [sub_path for sub_path in sub_paths if sub_path.is_dir()]\n",
    "    filepaths = [sub_path for sub_path in sub_paths if not sub_path in sub_dirs]\n",
    "    indent_str = \" \" * indent * level\n",
    "    for sub_dir in sub_dirs:\n",
    "        print(f\"{indent_str}{sub_dir.name}/\")\n",
    "        tree(sub_dir,  level + 1, indent)\n",
    "    for filepath in filepaths[:max_files]:\n",
    "        print(f\"{indent_str}{filepath.name}\")\n",
    "    if len(filepaths) > max_files:\n",
    "        print(f\"{indent_str}...\")\n",
    "\n",
    "print(tree(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1154d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12500, 12500, 12500, 12500)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def review_paths(dirpath):\n",
    "    return [str(path) for path in dirpath.glob(\"*.txt\")]\n",
    "\n",
    "train_pos = review_paths(path / \"train\" / \"pos\")\n",
    "train_neg = review_paths(path / \"train\" / \"neg\")\n",
    "test_valid_pos = review_paths(path / \"test\" / \"pos\")\n",
    "test_valid_neg = review_paths(path / \"test\" / \"neg\")\n",
    "\n",
    "len(train_pos), len(train_neg), len(test_valid_pos), len(test_valid_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "595a5a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(test_valid_pos)\n",
    "\n",
    "test_pos = test_valid_pos[:5000]\n",
    "test_neg = test_valid_neg[:5000]\n",
    "valid_pos = test_valid_pos[5000:]\n",
    "valid_neg = test_valid_neg[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0b2af30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'datasets/aclImdb/test/pos/7474_9.txt'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pos[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7075deba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imdb_dataset(filepaths_positive, filepaths_negative):\n",
    "    reviews = []\n",
    "    labels = []\n",
    "    for filepaths, label in ((filepaths_negative, 0), (filepaths_positive, 1)):\n",
    "        for filepath in filepaths:\n",
    "            with open(filepath) as review_file:\n",
    "                reviews.append(review_file.read())\n",
    "            labels.append(label)\n",
    "    return tf.data.Dataset.from_tensor_slices(\n",
    "        (tf.constant(reviews), tf.constant(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be439694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b\"Working with one of the best Shakespeare sources, this film manages to be creditable to it's source, whilst still appealing to a wider audience.<br /><br />Branagh steals the film from under Fishburne's nose, and there's a talented cast on good form.\", shape=(), dtype=string)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "\n",
      "tf.Tensor(b'Well...tremors I, the original started off in 1990 and i found the movie quite enjoyable to watch. however, they proceeded to make tremors II and III. Trust me, those movies started going downhill right after they finished the first one, i mean, ass blasters??? Now, only God himself is capable of answering the question \"why in Gods name would they create another one of these dumpster dives of a movie?\" Tremors IV cannot be considered a bad movie, in fact it cannot be even considered an epitome of a bad movie, for it lives up to more than that. As i attempted to sit though it, i noticed that my eyes started to bleed, and i hoped profusely that the little girl from the ring would crawl through the TV and kill me. did they really think that dressing the people who had stared in the other movies up as though they we\\'re from the wild west would make the movie (with the exact same occurrences) any better? honestly, i would never suggest buying this movie, i mean, there are cheaper ways to find things that burn well.', shape=(), dtype=string)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "\n",
      "tf.Tensor(b\"Ouch! This one was a bit painful to sit through. It has a cute and amusing premise, but it all goes to hell from there. Matthew Modine is almost always pedestrian and annoying, and he does not disappoint in this one. Deborah Kara Unger and John Neville turned in surprisingly decent performances. Alan Bates and Jennifer Tilly, among others, played it way over the top. I know that's the way the parts were written, and it's hard to blame actors, when the script and director have them do such schlock. If you're going to have outrageous characters, that's OK, but you gotta have good material to make it work. It didn't here. Run away screaming from this movie if at all possible.\", shape=(), dtype=string)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for X, y in imdb_dataset(train_pos, train_neg).take(3):\n",
    "    print(X)\n",
    "    print(y)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "abea0cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.9 s ¬± 0 ns per loop (mean ¬± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%timeit -r1 for X, y in imdb_dataset(train_pos, train_neg).repeat(10): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bea2b5",
   "metadata": {},
   "source": [
    "It takes about 17 seconds to load the dataset and go through it 10 times.\n",
    "\n",
    "But let's pretend the dataset does not fit in memory, just to make things more interesting. Luckily, each review fits on just one line (they use <br /> to indicate line breaks), so we can read the reviews using a TextLineDataset. If they didn't we would have to preprocess the input files (e.g., converting them to TFRecords). For very large datasets, it would make sense to use a tool like Apache Beam for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "28eff834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 s ¬± 0 ns per loop (mean ¬± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "def imdb_dataset(filepaths_positive, filepaths_negative, n_read_threads=5):\n",
    "    dataset_neg = tf.data.TextLineDataset(filepaths_negative,\n",
    "                                          num_parallel_reads=n_read_threads)\n",
    "    dataset_neg = dataset_neg.map(lambda review: (review, 0))\n",
    "    dataset_pos = tf.data.TextLineDataset(filepaths_positive,\n",
    "                                          num_parallel_reads=n_read_threads)\n",
    "    dataset_pos = dataset_pos.map(lambda review: (review, 1))\n",
    "    return tf.data.Dataset.concatenate(dataset_pos, dataset_neg)\n",
    "\n",
    "%timeit -r1 for X, y in imdb_dataset(train_pos, train_neg).repeat(10): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ed73ff",
   "metadata": {},
   "source": [
    "Now it takes about 33 seconds to go through the dataset 10 times. That's much slower, essentially because the dataset is not cached in RAM, so it must be reloaded at each epoch. If you add .cache() just before .repeat(10), you will see that this implementation will be about as fast as the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0dd230b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_with_label(fp, n_parse_threads=5):\n",
    "  label = tf.cast(tf.strings.regex_full_match(fp, '.*/pos/.*'), tf.int32)\n",
    "  return tf.data.TextLineDataset(fp).map(lambda line: (line, label), num_parallel_calls=n_parse_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "61e3ab3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def imdb_reader_dataset(filepaths,\n",
    "                       n_readers=5,\n",
    "                       n_parse_threads=5,\n",
    "                       shuffle_buffer_size=10000,\n",
    "                       seed=42,\n",
    "                       batch_size=32):\n",
    "  dataset = tf.data.Dataset.list_files(filepaths, seed=seed)\n",
    "  dataset = dataset.interleave(\n",
    "    lambda filepath: load_dataset_with_label(filepath),\n",
    "    cycle_length=n_readers,\n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    "  )\n",
    "  dataset = dataset.shuffle(shuffle_buffer_size, seed=seed)\n",
    "  return dataset.batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "00dd8426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.3 s ¬± 0 ns per loop (mean ¬± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = imdb_reader_dataset(train_pos + train_neg)\n",
    "%timeit -r1 for X in dataset.repeat(10): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "61972677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.66 s ¬± 0 ns per loop (mean ¬± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "training_dataset = imdb_reader_dataset(train_pos + train_neg)\n",
    "%timeit -r1 for X in training_dataset.cache().repeat(10): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "dedb5eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = imdb_reader_dataset(train_pos + train_neg)\n",
    "training_sample = training_dataset.take(500).map(lambda review, label: review)\n",
    "\n",
    "text_vectorization_layer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=1000,\n",
    "    output_mode=\"tf_idf\",\n",
    ")\n",
    "text_vectorization_layer.adapt(training_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "cdbc232f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]', 'the', 'a', 'and', 'of', 'to', 'is', 'in', 'it', 'i']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorization_layer.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8939e23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 8s 8ms/step - loss: 0.4415 - accuracy: 0.8202 - val_loss: 0.3648 - val_accuracy: 0.8462\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 8s 8ms/step - loss: 0.3592 - accuracy: 0.8558 - val_loss: 0.4355 - val_accuracy: 0.8243\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 8s 8ms/step - loss: 0.3111 - accuracy: 0.8727 - val_loss: 0.3541 - val_accuracy: 0.8507\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 8s 8ms/step - loss: 0.2546 - accuracy: 0.8980 - val_loss: 0.4480 - val_accuracy: 0.8245\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 7s 7ms/step - loss: 0.1894 - accuracy: 0.9264 - val_loss: 0.3844 - val_accuracy: 0.8490\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x31cf250c0>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "validation_dataset = imdb_reader_dataset(valid_pos + valid_neg)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    text_vectorization_layer,\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "model.fit(training_dataset, epochs=5, validation_data=validation_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4a077ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 9s 10ms/step - loss: 0.4847 - accuracy: 0.7595 - val_loss: 0.3482 - val_accuracy: 0.8545\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 8s 9ms/step - loss: 0.3441 - accuracy: 0.8515 - val_loss: 0.3820 - val_accuracy: 0.8253\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 9s 9ms/step - loss: 0.3242 - accuracy: 0.8620 - val_loss: 0.3181 - val_accuracy: 0.8611\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 8s 9ms/step - loss: 0.3150 - accuracy: 0.8641 - val_loss: 0.3342 - val_accuracy: 0.8527\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 8s 8ms/step - loss: 0.3090 - accuracy: 0.8680 - val_loss: 0.3286 - val_accuracy: 0.8555\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x31bafdb70>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "def compute_mean_embedding(inputs):\n",
    "    not_pad = tf.math.count_nonzero(inputs, axis=-1)\n",
    "    n_words = tf.math.count_nonzero(not_pad, axis=-1, keepdims=True)    \n",
    "    sqrt_n_words = tf.math.sqrt(tf.cast(n_words, tf.float32))\n",
    "    return tf.reduce_sum(inputs, axis=1) / sqrt_n_words\n",
    "\n",
    "another_example = tf.constant([[[1., 2., 3.], [4., 5., 0.], [0., 0., 0.]],\n",
    "                               [[6., 0., 0.], [0., 0., 0.], [0., 0., 0.]]])\n",
    "compute_mean_embedding(another_example)\n",
    "\n",
    "\n",
    "embedding_size = 20\n",
    "max_tokens = 1000\n",
    "\n",
    "text_vectorization_layer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    ")\n",
    "\n",
    "training_dataset = imdb_reader_dataset(train_pos + train_neg)\n",
    "training_sample = training_dataset.take(1000)\n",
    "adapt_sample = training_sample.map(lambda review, label: review)\n",
    "\n",
    "validation_dataset = imdb_reader_dataset(valid_pos + valid_neg).take(1000)\n",
    "\n",
    "text_vectorization_layer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=1000,\n",
    "    output_mode=\"int\",\n",
    ")\n",
    "text_vectorization_layer.adapt(adapt_sample)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    text_vectorization_layer,\n",
    "    tf.keras.layers.Embedding(input_dim=max_tokens,\n",
    "                              output_dim=embedding_size,\n",
    "                              mask_zero=True),  # <pad> tokens => zero vectors\n",
    "    tf.keras.layers.Lambda(compute_mean_embedding),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit(training_sample, epochs=5, validation_data=validation_dataset)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
